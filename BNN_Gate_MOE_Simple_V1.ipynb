{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAoyjQ4wA97RV5Zp0GzVgA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanmhmdi/Moe-llm-edge-computing/blob/main/BNN_Gate_MOE_Simple_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocgl-i2s3Uy7",
        "outputId": "e18d88d5-c9e4-4424-f4a3-8195cd7d5454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch 1 [0/60000] Loss: 256.7855 (Pred Loss: 2.3425, KL: 254442.9375)\n",
            "Train Epoch 1 [6400/60000] Loss: 244.9105 (Pred Loss: 0.2305, KL: 244680.0312)\n",
            "Train Epoch 1 [12800/60000] Loss: 235.1261 (Pred Loss: 0.1693, KL: 234956.7656)\n",
            "Train Epoch 1 [19200/60000] Loss: 225.5084 (Pred Loss: 0.2246, KL: 225283.8438)\n",
            "Train Epoch 1 [25600/60000] Loss: 215.9463 (Pred Loss: 0.2817, KL: 215664.5625)\n",
            "Train Epoch 1 [32000/60000] Loss: 206.2173 (Pred Loss: 0.1092, KL: 206108.0938)\n",
            "Train Epoch 1 [38400/60000] Loss: 196.8170 (Pred Loss: 0.1949, KL: 196622.0938)\n",
            "Train Epoch 1 [44800/60000] Loss: 187.3491 (Pred Loss: 0.1336, KL: 187215.5000)\n",
            "Train Epoch 1 [51200/60000] Loss: 178.1880 (Pred Loss: 0.2917, KL: 177896.3438)\n",
            "Train Epoch 1 [57600/60000] Loss: 168.7834 (Pred Loss: 0.1087, KL: 168674.6562)\n",
            "Train Epoch 1 Average Loss: 209.6124, Accuracy: 93.14%\n",
            "\n",
            "Test set: Average loss: 165.3168, Accuracy: 96.30%\n",
            "\n",
            "Train Epoch 2 [0/60000] Loss: 165.2966 (Pred Loss: 0.1009, KL: 165195.7344)\n",
            "Train Epoch 2 [6400/60000] Loss: 156.3221 (Pred Loss: 0.2023, KL: 156119.7500)\n",
            "Train Epoch 2 [12800/60000] Loss: 147.1878 (Pred Loss: 0.0219, KL: 147165.8750)\n",
            "Train Epoch 2 [19200/60000] Loss: 138.6631 (Pred Loss: 0.3085, KL: 138354.6250)\n",
            "Train Epoch 2 [25600/60000] Loss: 129.7595 (Pred Loss: 0.0714, KL: 129688.1406)\n",
            "Train Epoch 2 [32000/60000] Loss: 121.2566 (Pred Loss: 0.0739, KL: 121182.6875)\n",
            "Train Epoch 2 [38400/60000] Loss: 112.9483 (Pred Loss: 0.0928, KL: 112855.5234)\n",
            "Train Epoch 2 [44800/60000] Loss: 104.8564 (Pred Loss: 0.1266, KL: 104729.7969)\n",
            "Train Epoch 2 [51200/60000] Loss: 96.9783 (Pred Loss: 0.1514, KL: 96826.9766)\n",
            "Train Epoch 2 [57600/60000] Loss: 89.1675 (Pred Loss: 0.0174, KL: 89150.1094)\n",
            "Train Epoch 2 Average Loss: 124.6027, Accuracy: 97.04%\n",
            "\n",
            "Test set: Average loss: 86.3873, Accuracy: 96.98%\n",
            "\n",
            "Train Epoch 3 [0/60000] Loss: 86.3541 (Pred Loss: 0.0571, KL: 86297.0156)\n",
            "Train Epoch 3 [6400/60000] Loss: 79.0004 (Pred Loss: 0.0314, KL: 78969.0312)\n",
            "Train Epoch 3 [12800/60000] Loss: 71.9567 (Pred Loss: 0.0258, KL: 71930.8906)\n",
            "Train Epoch 3 [19200/60000] Loss: 65.1963 (Pred Loss: 0.0152, KL: 65181.1641)\n",
            "Train Epoch 3 [25600/60000] Loss: 58.8175 (Pred Loss: 0.0579, KL: 58759.5938)\n",
            "Train Epoch 3 [32000/60000] Loss: 52.7754 (Pred Loss: 0.0815, KL: 52693.9023)\n",
            "Train Epoch 3 [38400/60000] Loss: 47.0445 (Pred Loss: 0.0839, KL: 46960.6367)\n",
            "Train Epoch 3 [44800/60000] Loss: 41.6284 (Pred Loss: 0.0515, KL: 41576.8555)\n",
            "Train Epoch 3 [51200/60000] Loss: 36.5609 (Pred Loss: 0.0040, KL: 36556.8750)\n",
            "Train Epoch 3 [57600/60000] Loss: 31.9748 (Pred Loss: 0.0326, KL: 31942.2363)\n",
            "Train Epoch 3 Average Loss: 55.8971, Accuracy: 98.05%\n",
            "\n",
            "Test set: Average loss: 30.3676, Accuracy: 97.34%\n",
            "\n",
            "Train Epoch 4 [0/60000] Loss: 30.3405 (Pred Loss: 0.0565, KL: 30284.0332)\n",
            "Train Epoch 4 [6400/60000] Loss: 26.1799 (Pred Loss: 0.0097, KL: 26170.2324)\n",
            "Train Epoch 4 [12800/60000] Loss: 22.5097 (Pred Loss: 0.0841, KL: 22425.6465)\n",
            "Train Epoch 4 [19200/60000] Loss: 19.0818 (Pred Loss: 0.0349, KL: 19046.8672)\n",
            "Train Epoch 4 [25600/60000] Loss: 16.0861 (Pred Loss: 0.0545, KL: 16031.6250)\n",
            "Train Epoch 4 [32000/60000] Loss: 13.4071 (Pred Loss: 0.0489, KL: 13358.2383)\n",
            "Train Epoch 4 [38400/60000] Loss: 11.0740 (Pred Loss: 0.0496, KL: 11024.4434)\n",
            "Train Epoch 4 [44800/60000] Loss: 9.1399 (Pred Loss: 0.0689, KL: 9070.9766)\n",
            "Train Epoch 4 [51200/60000] Loss: 7.4055 (Pred Loss: 0.0297, KL: 7375.7402)\n",
            "Train Epoch 4 [57600/60000] Loss: 5.9503 (Pred Loss: 0.0165, KL: 5933.7505)\n",
            "Train Epoch 4 Average Loss: 15.4748, Accuracy: 98.47%\n",
            "\n",
            "Test set: Average loss: 5.5315, Accuracy: 97.48%\n",
            "\n",
            "Train Epoch 5 [0/60000] Loss: 5.4970 (Pred Loss: 0.0494, KL: 5447.6055)\n",
            "Train Epoch 5 [6400/60000] Loss: 4.3451 (Pred Loss: 0.0271, KL: 4317.9629)\n",
            "Train Epoch 5 [12800/60000] Loss: 3.4273 (Pred Loss: 0.0410, KL: 3386.2893)\n",
            "Train Epoch 5 [19200/60000] Loss: 2.6776 (Pred Loss: 0.0490, KL: 2628.5969)\n",
            "Train Epoch 5 [25600/60000] Loss: 2.0479 (Pred Loss: 0.0245, KL: 2023.3660)\n",
            "Train Epoch 5 [32000/60000] Loss: 1.5561 (Pred Loss: 0.0121, KL: 1543.9933)\n",
            "Train Epoch 5 [38400/60000] Loss: 1.2020 (Pred Loss: 0.0271, KL: 1174.9102)\n",
            "Train Epoch 5 [44800/60000] Loss: 0.8913 (Pred Loss: 0.0018, KL: 889.5803)\n",
            "Train Epoch 5 [51200/60000] Loss: 0.7089 (Pred Loss: 0.0337, KL: 675.2231)\n",
            "Train Epoch 5 [57600/60000] Loss: 0.6404 (Pred Loss: 0.1257, KL: 514.7329)\n",
            "Train Epoch 5 Average Loss: 2.1429, Accuracy: 98.81%\n",
            "\n",
            "Test set: Average loss: 0.5382, Accuracy: 97.97%\n",
            "\n",
            "Train Epoch 6 [0/60000] Loss: 0.5104 (Pred Loss: 0.0434, KL: 466.9262)\n",
            "Train Epoch 6 [6400/60000] Loss: 0.4390 (Pred Loss: 0.0724, KL: 366.6180)\n",
            "Train Epoch 6 [12800/60000] Loss: 0.2964 (Pred Loss: 0.0060, KL: 290.3331)\n",
            "Train Epoch 6 [19200/60000] Loss: 0.2520 (Pred Loss: 0.0164, KL: 235.6499)\n",
            "Train Epoch 6 [25600/60000] Loss: 0.2132 (Pred Loss: 0.0151, KL: 198.0774)\n",
            "Train Epoch 6 [32000/60000] Loss: 0.1883 (Pred Loss: 0.0193, KL: 168.9819)\n",
            "Train Epoch 6 [38400/60000] Loss: 0.1887 (Pred Loss: 0.0351, KL: 153.5158)\n",
            "Train Epoch 6 [44800/60000] Loss: 0.2075 (Pred Loss: 0.0717, KL: 135.7750)\n",
            "Train Epoch 6 [51200/60000] Loss: 0.1580 (Pred Loss: 0.0319, KL: 126.0977)\n",
            "Train Epoch 6 [57600/60000] Loss: 0.1401 (Pred Loss: 0.0240, KL: 116.1647)\n",
            "Train Epoch 6 Average Loss: 0.2455, Accuracy: 99.01%\n",
            "\n",
            "Test set: Average loss: 0.1926, Accuracy: 97.80%\n",
            "\n",
            "Train Epoch 7 [0/60000] Loss: 0.1414 (Pred Loss: 0.0288, KL: 112.6439)\n",
            "Train Epoch 7 [6400/60000] Loss: 0.1343 (Pred Loss: 0.0128, KL: 121.5194)\n",
            "Train Epoch 7 [12800/60000] Loss: 0.1099 (Pred Loss: 0.0010, KL: 108.8547)\n",
            "Train Epoch 7 [19200/60000] Loss: 0.1453 (Pred Loss: 0.0444, KL: 100.9444)\n",
            "Train Epoch 7 [25600/60000] Loss: 0.0978 (Pred Loss: 0.0026, KL: 95.1658)\n",
            "Train Epoch 7 [32000/60000] Loss: 0.0983 (Pred Loss: 0.0039, KL: 94.3645)\n",
            "Train Epoch 7 [38400/60000] Loss: 0.1286 (Pred Loss: 0.0329, KL: 95.6646)\n",
            "Train Epoch 7 [44800/60000] Loss: 0.1322 (Pred Loss: 0.0441, KL: 88.1282)\n",
            "Train Epoch 7 [51200/60000] Loss: 0.1738 (Pred Loss: 0.0909, KL: 82.9428)\n",
            "Train Epoch 7 [57600/60000] Loss: 0.0880 (Pred Loss: 0.0091, KL: 78.9032)\n",
            "Train Epoch 7 Average Loss: 0.1247, Accuracy: 99.14%\n",
            "\n",
            "Test set: Average loss: 0.1622, Accuracy: 97.69%\n",
            "\n",
            "Train Epoch 8 [0/60000] Loss: 0.1481 (Pred Loss: 0.0707, KL: 77.3761)\n",
            "Train Epoch 8 [6400/60000] Loss: 0.0826 (Pred Loss: 0.0086, KL: 73.9411)\n",
            "Train Epoch 8 [12800/60000] Loss: 0.0897 (Pred Loss: 0.0191, KL: 70.6059)\n",
            "Train Epoch 8 [19200/60000] Loss: 0.0863 (Pred Loss: 0.0183, KL: 68.0414)\n",
            "Train Epoch 8 [25600/60000] Loss: 0.0894 (Pred Loss: 0.0147, KL: 74.6686)\n",
            "Train Epoch 8 [32000/60000] Loss: 0.0884 (Pred Loss: 0.0035, KL: 84.9551)\n",
            "Train Epoch 8 [38400/60000] Loss: 0.1347 (Pred Loss: 0.0471, KL: 87.5926)\n",
            "Train Epoch 8 [44800/60000] Loss: 0.0791 (Pred Loss: 0.0007, KL: 78.3219)\n",
            "Train Epoch 8 [51200/60000] Loss: 0.0741 (Pred Loss: 0.0023, KL: 71.8308)\n",
            "Train Epoch 8 [57600/60000] Loss: 0.0796 (Pred Loss: 0.0125, KL: 67.1137)\n",
            "Train Epoch 8 Average Loss: 0.0983, Accuracy: 99.22%\n",
            "\n",
            "Test set: Average loss: 0.1471, Accuracy: 97.92%\n",
            "\n",
            "Train Epoch 9 [0/60000] Loss: 0.0680 (Pred Loss: 0.0024, KL: 65.5420)\n",
            "Train Epoch 9 [6400/60000] Loss: 0.1219 (Pred Loss: 0.0599, KL: 62.0747)\n",
            "Train Epoch 9 [12800/60000] Loss: 0.0723 (Pred Loss: 0.0087, KL: 63.5905)\n",
            "Train Epoch 9 [19200/60000] Loss: 0.0798 (Pred Loss: 0.0171, KL: 62.6773)\n",
            "Train Epoch 9 [25600/60000] Loss: 0.0575 (Pred Loss: 0.0003, KL: 57.1799)\n",
            "Train Epoch 9 [32000/60000] Loss: 0.0610 (Pred Loss: 0.0010, KL: 59.9645)\n",
            "Train Epoch 9 [38400/60000] Loss: 0.0560 (Pred Loss: 0.0014, KL: 54.6074)\n",
            "Train Epoch 9 [44800/60000] Loss: 0.0641 (Pred Loss: 0.0128, KL: 51.3591)\n",
            "Train Epoch 9 [51200/60000] Loss: 0.0495 (Pred Loss: 0.0006, KL: 48.8280)\n",
            "Train Epoch 9 [57600/60000] Loss: 0.1061 (Pred Loss: 0.0421, KL: 63.9744)\n",
            "Train Epoch 9 Average Loss: 0.0781, Accuracy: 99.37%\n",
            "\n",
            "Test set: Average loss: 0.1427, Accuracy: 98.00%\n",
            "\n",
            "Train Epoch 10 [0/60000] Loss: 0.0750 (Pred Loss: 0.0127, KL: 62.2803)\n",
            "Train Epoch 10 [6400/60000] Loss: 0.0639 (Pred Loss: 0.0005, KL: 63.4201)\n",
            "Train Epoch 10 [12800/60000] Loss: 0.0595 (Pred Loss: 0.0034, KL: 56.0357)\n",
            "Train Epoch 10 [19200/60000] Loss: 0.0526 (Pred Loss: 0.0015, KL: 51.1349)\n",
            "Train Epoch 10 [25600/60000] Loss: 0.0740 (Pred Loss: 0.0258, KL: 48.2054)\n",
            "Train Epoch 10 [32000/60000] Loss: 0.3323 (Pred Loss: 0.2869, KL: 45.4606)\n",
            "Train Epoch 10 [38400/60000] Loss: 0.0482 (Pred Loss: 0.0048, KL: 43.3706)\n",
            "Train Epoch 10 [44800/60000] Loss: 0.0422 (Pred Loss: 0.0006, KL: 41.6057)\n",
            "Train Epoch 10 [51200/60000] Loss: 0.0467 (Pred Loss: 0.0048, KL: 41.9550)\n",
            "Train Epoch 10 [57600/60000] Loss: 0.0478 (Pred Loss: 0.0062, KL: 41.6554)\n",
            "Train Epoch 10 Average Loss: 0.0690, Accuracy: 99.41%\n",
            "\n",
            "Test set: Average loss: 0.1267, Accuracy: 98.06%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# -------------------------\n",
        "# Bayesian Linear Layer\n",
        "# -------------------------\n",
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, prior_mu=0.0, prior_sigma=1.0):\n",
        "        super(BayesianLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # Learnable variational parameters for weights and biases.\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_features))\n",
        "\n",
        "        self.prior_mu = prior_mu\n",
        "        self.prior_sigma = prior_sigma\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1.0 / self.in_features ** 0.5\n",
        "        self.weight_mu.data.uniform_(-stdv, stdv)\n",
        "        # Initialize rho to negative values to start with small sigma (via softplus)\n",
        "        self.weight_rho.data.fill_(-3)\n",
        "        self.bias_mu.data.uniform_(-stdv, stdv)\n",
        "        self.bias_rho.data.fill_(-3)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Transform rho to sigma using softplus\n",
        "        weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
        "        bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
        "        # Sample epsilon from standard normal distribution\n",
        "        weight_eps = torch.randn_like(self.weight_mu)\n",
        "        bias_eps = torch.randn_like(self.bias_mu)\n",
        "        # Reparameterization trick: sample = mu + sigma * epsilon\n",
        "        weight = self.weight_mu + weight_sigma * weight_eps\n",
        "        bias = self.bias_mu + bias_sigma * bias_eps\n",
        "\n",
        "        # Compute output and KL divergence for this layer.\n",
        "        output = F.linear(input, weight, bias)\n",
        "        self.kl = self.kl_divergence(self.weight_mu, weight_sigma) + self.kl_divergence(self.bias_mu, bias_sigma)\n",
        "        return output\n",
        "\n",
        "    def kl_divergence(self, mu, sigma):\n",
        "        \"\"\"\n",
        "        Closed-form KL divergence between q(w) = N(mu, sigma^2)\n",
        "        and p(w) = N(prior_mu, prior_sigma^2):\n",
        "            KL = log(prior_sigma / sigma) + (sigma^2 + (mu - prior_mu)^2) / (2 * prior_sigma^2) - 1/2\n",
        "        \"\"\"\n",
        "        prior_sigma = self.prior_sigma\n",
        "        prior_mu = self.prior_mu\n",
        "        kl = torch.log(prior_sigma / sigma) + (sigma.pow(2) + (mu - prior_mu).pow(2)) / (2 * prior_sigma**2) - 0.5\n",
        "        return kl.sum()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Bayesian Gate Network\n",
        "# -------------------------\n",
        "class BayesianGate(nn.Module):\n",
        "    def __init__(self, input_dim, num_experts, hidden_dim=128):\n",
        "        super(BayesianGate, self).__init__()\n",
        "        # Two BayesianLinear layers with a non-linearity.\n",
        "        self.fc1 = BayesianLinear(input_dim, hidden_dim)\n",
        "        self.fc2 = BayesianLinear(hidden_dim, num_experts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        logits = self.fc2(x)\n",
        "        # Apply softmax to get a probability distribution over experts.\n",
        "        gate_weights = F.softmax(logits, dim=-1)\n",
        "        # Sum the KL divergence from both Bayesian layers.\n",
        "        kl = self.fc1.kl + self.fc2.kl\n",
        "        return gate_weights, kl\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Deterministic Expert Network\n",
        "# -------------------------\n",
        "class Expert(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=256):\n",
        "        super(Expert, self).__init__()\n",
        "        # A simple two-layer MLP for classification.\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Mixture of Experts (MoE) Model\n",
        "# -------------------------\n",
        "class MixtureOfExperts(nn.Module):\n",
        "    def __init__(self, input_dim, num_experts, output_dim,\n",
        "                 expert_hidden_dim=256, gate_hidden_dim=128):\n",
        "        super(MixtureOfExperts, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        # Create a list (ModuleList) of deterministic expert networks.\n",
        "        self.experts = nn.ModuleList([\n",
        "            Expert(input_dim, output_dim, hidden_dim=expert_hidden_dim)\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "        # Create the Bayesian gating network.\n",
        "        self.gate = BayesianGate(input_dim, num_experts, hidden_dim=gate_hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute gate weights and its KL divergence.\n",
        "        gate_weights, gate_kl = self.gate(x)  # gate_weights: (batch_size, num_experts)\n",
        "\n",
        "        # Get output from each expert.\n",
        "        expert_outputs = [expert(x) for expert in self.experts]  # Each: (batch_size, output_dim)\n",
        "        # Stack expert outputs into shape: (batch_size, num_experts, output_dim)\n",
        "        expert_outputs = torch.stack(expert_outputs, dim=1)\n",
        "\n",
        "        # Reshape gate weights to match the expert outputs.\n",
        "        gate_weights = gate_weights.unsqueeze(-1)  # (batch_size, num_experts, 1)\n",
        "        # Weighted sum over experts.\n",
        "        output = torch.sum(gate_weights * expert_outputs, dim=1)\n",
        "        return output, gate_kl\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Training and Testing Functions\n",
        "# -------------------------\n",
        "def train(model, device, train_loader, optimizer, epoch, kl_weight):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Flatten the MNIST images into vectors of size 28*28 = 784.\n",
        "        data = data.view(data.size(0), -1).to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, gate_kl = model(data)\n",
        "        # Compute cross-entropy loss on predictions.\n",
        "        loss_pred = F.cross_entropy(output, target)\n",
        "        # Total loss includes the KL divergence weighted by kl_weight.\n",
        "        loss = loss_pred + kl_weight * gate_kl\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * data.size(0)\n",
        "        pred = output.argmax(dim=1)\n",
        "        total_correct += pred.eq(target).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Train Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] \"\n",
        "                  f\"Loss: {loss.item():.4f} (Pred Loss: {loss_pred.item():.4f}, KL: {gate_kl.item():.4f})\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "    accuracy = total_correct / len(train_loader.dataset)\n",
        "    print(f\"Train Epoch {epoch} Average Loss: {avg_loss:.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, kl_weight):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data = data.view(data.size(0), -1).to(device)\n",
        "            target = target.to(device)\n",
        "            output, gate_kl = model(data)\n",
        "            loss_pred = F.cross_entropy(output, target)\n",
        "            loss = loss_pred + kl_weight * gate_kl\n",
        "            total_loss += loss.item() * data.size(0)\n",
        "            pred = output.argmax(dim=1)\n",
        "            total_correct += pred.eq(target).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader.dataset)\n",
        "    accuracy = total_correct / len(test_loader.dataset)\n",
        "    print(f\"\\nTest set: Average loss: {avg_loss:.4f}, Accuracy: {accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Main Function\n",
        "# -------------------------\n",
        "\n",
        "# Hyperparameters:\n",
        "batch_size = 64\n",
        "test_batch_size = 1000\n",
        "epochs = 10         # You may increase to get better performance.\n",
        "learning_rate = 1e-3\n",
        "kl_weight = 0.001   # Adjust weight for the KL divergence regularization.\n",
        "\n",
        "# Define device: use CUDA if available.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Prepare MNIST dataset and DataLoader with standard normalization.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize MNIST dataset with mean and std. You can also flatten later.\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
        "\n",
        "# Input dimension is 28*28 = 784; number of classes is 10.\n",
        "input_dim = 28 * 28\n",
        "output_dim = 10\n",
        "num_experts = 2  # You can experiment with more experts.\n",
        "\n",
        "# Initialize the Mixture of Experts model.\n",
        "model = MixtureOfExperts(input_dim, num_experts, output_dim,\n",
        "                          expert_hidden_dim=256, gate_hidden_dim=128).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop.\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch, kl_weight)\n",
        "    test(model, device, test_loader, kl_weight)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_uncertainty(model, data, num_samples=30):\n",
        "    \"\"\"\n",
        "    Runs multiple forward passes on the data using the Bayesian gate's stochasticity\n",
        "    and returns the mean predictions and uncertainty (e.g., standard deviation).\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained MoE model.\n",
        "        data (Tensor): Input tensor (e.g., batch of flattened MNIST images).\n",
        "        num_samples (int): Number of Monte Carlo samples.\n",
        "\n",
        "    Returns:\n",
        "        avg_output (Tensor): Averaged predictions across samples.\n",
        "        uncertainty (Tensor): Standard deviation across predictions.\n",
        "    \"\"\"\n",
        "    model.eval()  # Ensure model is in evaluation mode\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_samples):\n",
        "            # Each forward pass samples new weights for the Bayesian layers.\n",
        "            output, _ = model(data)\n",
        "            preds.append(output)\n",
        "\n",
        "    # Stack the predictions to shape (num_samples, batch_size, num_classes)\n",
        "    preds = torch.stack(preds)\n",
        "    # Average predictions along the sample axis.\n",
        "    avg_output = preds.mean(dim=0)\n",
        "    # Compute the uncertainty (e.g., standard deviation) across the samples.\n",
        "    uncertainty = preds.std(dim=0)\n",
        "    return avg_output, uncertainty\n",
        "\n",
        "# Suppose you have a batch of MNIST test data:\n",
        "data, target = next(iter(test_loader))\n",
        "data = data.view(data.size(0), -1).to(device)\n",
        "\n",
        "# Get the predictions and uncertainty using Monte Carlo sampling.\n",
        "mean_predictions, prediction_uncertainty = predict_with_uncertainty(model, data, num_samples=50)\n",
        "\n",
        "# You can now use mean_predictions for decision-making and prediction_uncertainty\n",
        "# for risk evaluation or further analysis.\n",
        "print(\"Mean Predictions:\", mean_predictions)\n",
        "print(\"Prediction Uncertainty:\", prediction_uncertainty)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KqEObjr3Y6W",
        "outputId": "325f1b9e-ccf6-4ac9-a32a-fc70604cf285"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Predictions: tensor([[ -9.5471, -13.9054,  -7.6679,  ...,  13.6950, -10.8544,  -5.2255],\n",
            "        [-12.1189,  -2.1980,  18.3954,  ..., -23.8359,  -6.2128, -32.9885],\n",
            "        [-17.3906,   7.7076,  -0.2277,  ...,  -4.0017,  -6.7653, -18.7809],\n",
            "        ...,\n",
            "        [ 29.8338, -31.1928,  -0.6697,  ...,  -6.5207, -30.6078, -13.4928],\n",
            "        [-10.5167, -18.5244,   0.5654,  ...,  -7.8078,  10.4129, -10.8197],\n",
            "        [ -8.3470, -15.1543,  -8.8521,  ...,   6.5622, -12.0683,   9.1366]])\n",
            "Prediction Uncertainty: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [1.3681, 1.7487, 0.3635,  ..., 0.4994, 0.4111, 1.3095],\n",
            "        [0.4162, 1.6816, 0.8238,  ..., 0.6839, 1.7301, 0.9185]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(prediction_uncertainty[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdnELSYu5xUz",
        "outputId": "6438d133-cf5d-476a-f4b7-cea7b23428ab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iCYMucxo6L0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}