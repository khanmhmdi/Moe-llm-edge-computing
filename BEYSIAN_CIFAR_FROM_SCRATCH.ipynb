{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNt+C2mRD4VEMUTt/0mFKAJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanmhmdi/Moe-llm-edge-computing/blob/main/BEYSIAN_CIFAR_FROM_SCRATCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Jlv0OygSvsR5",
        "outputId": "200e741b-97f6-4ad4-bf95-1ad470909c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,     1] ELBO Loss: 52.926 CE Loss: 0.041 KL Loss: 5288.494 Train Accuracy: 9.28%\n",
            "[1,    41] ELBO Loss: 2105.028 CE Loss: 0.606 KL Loss: 210442.245 Train Accuracy: 21.22%\n",
            "[2,     1] ELBO Loss: 52.237 CE Loss: 0.010 KL Loss: 5222.644 Train Accuracy: 27.34%\n",
            "[2,    41] ELBO Loss: 2078.369 CE Loss: 0.379 KL Loss: 207798.972 Train Accuracy: 31.93%\n",
            "[3,     1] ELBO Loss: 51.575 CE Loss: 0.009 KL Loss: 5156.583 Train Accuracy: 34.86%\n",
            "[3,    41] ELBO Loss: 2051.970 CE Loss: 0.338 KL Loss: 205163.152 Train Accuracy: 38.98%\n",
            "[4,     1] ELBO Loss: 50.917 CE Loss: 0.008 KL Loss: 5090.945 Train Accuracy: 42.68%\n",
            "[4,    41] ELBO Loss: 2025.761 CE Loss: 0.308 KL Loss: 202545.337 Train Accuracy: 43.85%\n",
            "[5,     1] ELBO Loss: 50.265 CE Loss: 0.007 KL Loss: 5025.753 Train Accuracy: 47.17%\n",
            "[5,    41] ELBO Loss: 1999.725 CE Loss: 0.287 KL Loss: 199943.899 Train Accuracy: 47.94%\n",
            "[6,     1] ELBO Loss: 49.616 CE Loss: 0.007 KL Loss: 4960.915 Train Accuracy: 45.70%\n",
            "[6,    41] ELBO Loss: 1973.823 CE Loss: 0.271 KL Loss: 197355.244 Train Accuracy: 50.77%\n",
            "[7,     1] ELBO Loss: 48.970 CE Loss: 0.007 KL Loss: 4896.351 Train Accuracy: 51.56%\n",
            "[7,    41] ELBO Loss: 1948.023 CE Loss: 0.260 KL Loss: 194776.369 Train Accuracy: 52.92%\n",
            "[8,     1] ELBO Loss: 48.326 CE Loss: 0.006 KL Loss: 4831.994 Train Accuracy: 57.42%\n",
            "[8,    41] ELBO Loss: 1922.300 CE Loss: 0.251 KL Loss: 192204.928 Train Accuracy: 54.61%\n",
            "[9,     1] ELBO Loss: 47.684 CE Loss: 0.006 KL Loss: 4767.797 Train Accuracy: 54.69%\n",
            "[9,    41] ELBO Loss: 1896.641 CE Loss: 0.248 KL Loss: 189639.254 Train Accuracy: 55.13%\n",
            "[10,     1] ELBO Loss: 47.043 CE Loss: 0.006 KL Loss: 4703.724 Train Accuracy: 54.20%\n",
            "[10,    41] ELBO Loss: 1871.021 CE Loss: 0.241 KL Loss: 187078.081 Train Accuracy: 56.72%\n",
            "[11,     1] ELBO Loss: 46.403 CE Loss: 0.006 KL Loss: 4639.751 Train Accuracy: 57.32%\n",
            "[11,    41] ELBO Loss: 1845.439 CE Loss: 0.234 KL Loss: 184520.543 Train Accuracy: 58.13%\n",
            "[12,     1] ELBO Loss: 45.765 CE Loss: 0.006 KL Loss: 4575.860 Train Accuracy: 56.64%\n",
            "[12,    41] ELBO Loss: 1819.888 CE Loss: 0.226 KL Loss: 181966.199 Train Accuracy: 59.23%\n",
            "[13,     1] ELBO Loss: 45.126 CE Loss: 0.006 KL Loss: 4512.044 Train Accuracy: 58.50%\n",
            "[13,    41] ELBO Loss: 1794.373 CE Loss: 0.226 KL Loss: 179414.760 Train Accuracy: 59.59%\n",
            "[14,     1] ELBO Loss: 44.488 CE Loss: 0.005 KL Loss: 4448.299 Train Accuracy: 64.65%\n",
            "[14,    41] ELBO Loss: 1768.883 CE Loss: 0.222 KL Loss: 176866.075 Train Accuracy: 60.20%\n",
            "[15,     1] ELBO Loss: 43.852 CE Loss: 0.005 KL Loss: 4384.620 Train Accuracy: 61.04%\n",
            "[15,    41] ELBO Loss: 1743.421 CE Loss: 0.220 KL Loss: 174320.017 Train Accuracy: 60.38%\n",
            "[16,     1] ELBO Loss: 43.215 CE Loss: 0.005 KL Loss: 4321.006 Train Accuracy: 62.01%\n",
            "[16,    41] ELBO Loss: 1717.982 CE Loss: 0.217 KL Loss: 171776.528 Train Accuracy: 61.19%\n",
            "[17,     1] ELBO Loss: 42.580 CE Loss: 0.005 KL Loss: 4257.458 Train Accuracy: 62.70%\n",
            "[17,    41] ELBO Loss: 1692.578 CE Loss: 0.221 KL Loss: 169235.760 Train Accuracy: 60.51%\n",
            "[18,     1] ELBO Loss: 41.945 CE Loss: 0.005 KL Loss: 4193.978 Train Accuracy: 63.09%\n",
            "[18,    41] ELBO Loss: 1667.200 CE Loss: 0.223 KL Loss: 166697.713 Train Accuracy: 59.99%\n",
            "[19,     1] ELBO Loss: 41.311 CE Loss: 0.005 KL Loss: 4130.568 Train Accuracy: 62.40%\n",
            "[19,    41] ELBO Loss: 1641.842 CE Loss: 0.218 KL Loss: 164162.464 Train Accuracy: 61.06%\n",
            "[20,     1] ELBO Loss: 40.678 CE Loss: 0.006 KL Loss: 4067.228 Train Accuracy: 58.11%\n",
            "[20,    41] ELBO Loss: 1616.520 CE Loss: 0.219 KL Loss: 161630.112 Train Accuracy: 61.27%\n",
            "[21,     1] ELBO Loss: 40.045 CE Loss: 0.005 KL Loss: 4003.963 Train Accuracy: 63.87%\n",
            "[21,    41] ELBO Loss: 1591.230 CE Loss: 0.221 KL Loss: 159100.868 Train Accuracy: 60.59%\n",
            "[22,     1] ELBO Loss: 39.413 CE Loss: 0.006 KL Loss: 3940.778 Train Accuracy: 59.96%\n",
            "[22,    41] ELBO Loss: 1565.964 CE Loss: 0.217 KL Loss: 156574.779 Train Accuracy: 61.45%\n",
            "[23,     1] ELBO Loss: 38.782 CE Loss: 0.005 KL Loss: 3877.674 Train Accuracy: 61.52%\n",
            "[23,    41] ELBO Loss: 1540.744 CE Loss: 0.224 KL Loss: 154052.061 Train Accuracy: 60.13%\n",
            "[24,     1] ELBO Loss: 38.152 CE Loss: 0.005 KL Loss: 3814.657 Train Accuracy: 59.57%\n",
            "[24,    41] ELBO Loss: 1515.553 CE Loss: 0.223 KL Loss: 151532.951 Train Accuracy: 60.43%\n",
            "[25,     1] ELBO Loss: 37.523 CE Loss: 0.006 KL Loss: 3751.733 Train Accuracy: 58.59%\n",
            "[25,    41] ELBO Loss: 1490.401 CE Loss: 0.226 KL Loss: 149017.496 Train Accuracy: 59.57%\n",
            "[26,     1] ELBO Loss: 36.894 CE Loss: 0.005 KL Loss: 3688.903 Train Accuracy: 61.33%\n",
            "[26,    41] ELBO Loss: 1465.283 CE Loss: 0.224 KL Loss: 146505.911 Train Accuracy: 60.14%\n",
            "[27,     1] ELBO Loss: 36.267 CE Loss: 0.006 KL Loss: 3626.171 Train Accuracy: 61.13%\n",
            "[27,    41] ELBO Loss: 1440.215 CE Loss: 0.230 KL Loss: 143998.451 Train Accuracy: 58.86%\n",
            "[28,     1] ELBO Loss: 35.641 CE Loss: 0.006 KL Loss: 3563.546 Train Accuracy: 60.06%\n",
            "[28,    41] ELBO Loss: 1415.189 CE Loss: 0.236 KL Loss: 141495.307 Train Accuracy: 57.81%\n",
            "[29,     1] ELBO Loss: 35.016 CE Loss: 0.005 KL Loss: 3501.035 Train Accuracy: 59.86%\n",
            "[29,    41] ELBO Loss: 1390.199 CE Loss: 0.232 KL Loss: 138996.760 Train Accuracy: 58.75%\n",
            "[30,     1] ELBO Loss: 34.392 CE Loss: 0.006 KL Loss: 3438.637 Train Accuracy: 56.74%\n",
            "[30,    41] ELBO Loss: 1365.264 CE Loss: 0.235 KL Loss: 136502.888 Train Accuracy: 58.20%\n",
            "[31,     1] ELBO Loss: 33.770 CE Loss: 0.006 KL Loss: 3376.365 Train Accuracy: 56.74%\n",
            "[31,    41] ELBO Loss: 1340.379 CE Loss: 0.238 KL Loss: 134014.103 Train Accuracy: 57.34%\n",
            "[32,     1] ELBO Loss: 33.148 CE Loss: 0.006 KL Loss: 3314.220 Train Accuracy: 58.98%\n",
            "[32,    41] ELBO Loss: 1315.546 CE Loss: 0.240 KL Loss: 131530.562 Train Accuracy: 57.49%\n",
            "[33,     1] ELBO Loss: 32.528 CE Loss: 0.006 KL Loss: 3252.212 Train Accuracy: 59.28%\n",
            "[33,    41] ELBO Loss: 1290.768 CE Loss: 0.241 KL Loss: 129052.627 Train Accuracy: 56.78%\n",
            "[34,     1] ELBO Loss: 31.910 CE Loss: 0.006 KL Loss: 3190.346 Train Accuracy: 52.05%\n",
            "[34,    41] ELBO Loss: 1266.051 CE Loss: 0.246 KL Loss: 126580.554 Train Accuracy: 56.32%\n",
            "[35,     1] ELBO Loss: 31.293 CE Loss: 0.006 KL Loss: 3128.634 Train Accuracy: 54.49%\n",
            "[35,    41] ELBO Loss: 1241.397 CE Loss: 0.250 KL Loss: 124114.672 Train Accuracy: 55.56%\n",
            "[36,     1] ELBO Loss: 30.677 CE Loss: 0.006 KL Loss: 3067.079 Train Accuracy: 53.61%\n",
            "[36,    41] ELBO Loss: 1216.804 CE Loss: 0.252 KL Loss: 121655.262 Train Accuracy: 55.58%\n",
            "[37,     1] ELBO Loss: 30.063 CE Loss: 0.006 KL Loss: 3005.691 Train Accuracy: 58.79%\n",
            "[37,    41] ELBO Loss: 1192.283 CE Loss: 0.257 KL Loss: 119202.657 Train Accuracy: 54.57%\n",
            "[38,     1] ELBO Loss: 29.451 CE Loss: 0.007 KL Loss: 2944.482 Train Accuracy: 52.64%\n",
            "[38,    41] ELBO Loss: 1167.833 CE Loss: 0.260 KL Loss: 116757.321 Train Accuracy: 53.71%\n",
            "[39,     1] ELBO Loss: 28.841 CE Loss: 0.006 KL Loss: 2883.457 Train Accuracy: 55.57%\n",
            "[39,    41] ELBO Loss: 1143.452 CE Loss: 0.258 KL Loss: 114319.451 Train Accuracy: 54.66%\n",
            "[40,     1] ELBO Loss: 28.233 CE Loss: 0.007 KL Loss: 2822.622 Train Accuracy: 53.81%\n",
            "[40,    41] ELBO Loss: 1119.158 CE Loss: 0.262 KL Loss: 111889.583 Train Accuracy: 53.61%\n",
            "[41,     1] ELBO Loss: 27.627 CE Loss: 0.007 KL Loss: 2761.997 Train Accuracy: 53.22%\n",
            "[41,    41] ELBO Loss: 1094.949 CE Loss: 0.269 KL Loss: 109468.053 Train Accuracy: 52.69%\n",
            "[42,     1] ELBO Loss: 27.022 CE Loss: 0.007 KL Loss: 2701.587 Train Accuracy: 54.49%\n",
            "[42,    41] ELBO Loss: 1070.822 CE Loss: 0.268 KL Loss: 107055.418 Train Accuracy: 52.54%\n",
            "[43,     1] ELBO Loss: 26.420 CE Loss: 0.006 KL Loss: 2641.403 Train Accuracy: 54.39%\n",
            "[43,    41] ELBO Loss: 1046.795 CE Loss: 0.275 KL Loss: 104652.036 Train Accuracy: 51.59%\n",
            "[44,     1] ELBO Loss: 25.822 CE Loss: 0.007 KL Loss: 2581.456 Train Accuracy: 50.88%\n",
            "[44,    41] ELBO Loss: 1022.859 CE Loss: 0.275 KL Loss: 102258.346 Train Accuracy: 51.56%\n",
            "[45,     1] ELBO Loss: 25.224 CE Loss: 0.007 KL Loss: 2521.763 Train Accuracy: 51.37%\n",
            "[45,    41] ELBO Loss: 999.024 CE Loss: 0.276 KL Loss: 99874.852 Train Accuracy: 51.21%\n",
            "[46,     1] ELBO Loss: 24.630 CE Loss: 0.007 KL Loss: 2462.327 Train Accuracy: 52.73%\n",
            "[46,    41] ELBO Loss: 975.300 CE Loss: 0.279 KL Loss: 97502.128 Train Accuracy: 50.18%\n",
            "[47,     1] ELBO Loss: 24.039 CE Loss: 0.007 KL Loss: 2403.174 Train Accuracy: 50.68%\n",
            "[47,    41] ELBO Loss: 951.693 CE Loss: 0.285 KL Loss: 95140.830 Train Accuracy: 49.42%\n",
            "[48,     1] ELBO Loss: 23.451 CE Loss: 0.008 KL Loss: 2344.313 Train Accuracy: 43.46%\n",
            "[48,    41] ELBO Loss: 928.202 CE Loss: 0.288 KL Loss: 92791.377 Train Accuracy: 49.38%\n",
            "[49,     1] ELBO Loss: 22.865 CE Loss: 0.007 KL Loss: 2285.754 Train Accuracy: 49.22%\n",
            "[49,    41] ELBO Loss: 904.835 CE Loss: 0.292 KL Loss: 90454.308 Train Accuracy: 47.78%\n",
            "[50,     1] ELBO Loss: 22.282 CE Loss: 0.007 KL Loss: 2227.516 Train Accuracy: 51.27%\n",
            "[50,    41] ELBO Loss: 881.599 CE Loss: 0.295 KL Loss: 88130.442 Train Accuracy: 47.55%\n",
            "[51,     1] ELBO Loss: 21.704 CE Loss: 0.007 KL Loss: 2169.615 Train Accuracy: 47.95%\n",
            "[51,    41] ELBO Loss: 858.503 CE Loss: 0.301 KL Loss: 85820.252 Train Accuracy: 46.60%\n",
            "[52,     1] ELBO Loss: 21.128 CE Loss: 0.007 KL Loss: 2112.072 Train Accuracy: 46.29%\n",
            "[52,    41] ELBO Loss: 835.549 CE Loss: 0.303 KL Loss: 83524.659 Train Accuracy: 45.85%\n",
            "[53,     1] ELBO Loss: 20.557 CE Loss: 0.008 KL Loss: 2054.894 Train Accuracy: 44.63%\n",
            "[53,    41] ELBO Loss: 812.742 CE Loss: 0.303 KL Loss: 81243.875 Train Accuracy: 45.96%\n",
            "[54,     1] ELBO Loss: 19.989 CE Loss: 0.008 KL Loss: 1998.107 Train Accuracy: 38.67%\n",
            "[54,    41] ELBO Loss: 790.100 CE Loss: 0.307 KL Loss: 78979.305 Train Accuracy: 44.79%\n",
            "[55,     1] ELBO Loss: 19.426 CE Loss: 0.008 KL Loss: 1941.728 Train Accuracy: 38.67%\n",
            "[55,    41] ELBO Loss: 767.625 CE Loss: 0.314 KL Loss: 76731.139 Train Accuracy: 43.52%\n",
            "[56,     1] ELBO Loss: 18.865 CE Loss: 0.008 KL Loss: 1885.774 Train Accuracy: 42.58%\n",
            "[56,    41] ELBO Loss: 745.326 CE Loss: 0.323 KL Loss: 74500.387 Train Accuracy: 41.98%\n",
            "[57,     1] ELBO Loss: 18.311 CE Loss: 0.008 KL Loss: 1830.271 Train Accuracy: 40.72%\n",
            "[57,    41] ELBO Loss: 723.203 CE Loss: 0.323 KL Loss: 72287.967 Train Accuracy: 41.95%\n",
            "[58,     1] ELBO Loss: 17.760 CE Loss: 0.008 KL Loss: 1775.233 Train Accuracy: 43.36%\n",
            "[58,    41] ELBO Loss: 701.273 CE Loss: 0.328 KL Loss: 70094.561 Train Accuracy: 40.69%\n",
            "[59,     1] ELBO Loss: 17.215 CE Loss: 0.008 KL Loss: 1720.680 Train Accuracy: 41.02%\n",
            "[59,    41] ELBO Loss: 679.539 CE Loss: 0.330 KL Loss: 67920.907 Train Accuracy: 39.96%\n",
            "[60,     1] ELBO Loss: 16.675 CE Loss: 0.009 KL Loss: 1666.642 Train Accuracy: 38.57%\n",
            "[60,    41] ELBO Loss: 658.023 CE Loss: 0.340 KL Loss: 65768.331 Train Accuracy: 38.51%\n",
            "[61,     1] ELBO Loss: 16.140 CE Loss: 0.008 KL Loss: 1613.140 Train Accuracy: 39.94%\n",
            "[61,    41] ELBO Loss: 636.717 CE Loss: 0.343 KL Loss: 63637.359 Train Accuracy: 37.35%\n",
            "[62,     1] ELBO Loss: 15.611 CE Loss: 0.009 KL Loss: 1560.190 Train Accuracy: 35.06%\n",
            "[62,    41] ELBO Loss: 615.640 CE Loss: 0.349 KL Loss: 61529.099 Train Accuracy: 36.46%\n",
            "[63,     1] ELBO Loss: 15.087 CE Loss: 0.009 KL Loss: 1507.825 Train Accuracy: 36.04%\n",
            "[63,    41] ELBO Loss: 594.802 CE Loss: 0.357 KL Loss: 59444.496 Train Accuracy: 34.88%\n",
            "[64,     1] ELBO Loss: 14.569 CE Loss: 0.009 KL Loss: 1456.068 Train Accuracy: 37.89%\n",
            "[64,    41] ELBO Loss: 574.204 CE Loss: 0.357 KL Loss: 57384.676 Train Accuracy: 34.66%\n",
            "[65,     1] ELBO Loss: 14.059 CE Loss: 0.010 KL Loss: 1404.938 Train Accuracy: 33.30%\n",
            "[65,    41] ELBO Loss: 553.870 CE Loss: 0.366 KL Loss: 55350.434 Train Accuracy: 32.68%\n",
            "[66,     1] ELBO Loss: 13.554 CE Loss: 0.010 KL Loss: 1354.467 Train Accuracy: 33.30%\n",
            "[66,    41] ELBO Loss: 533.799 CE Loss: 0.372 KL Loss: 53342.789 Train Accuracy: 31.54%\n",
            "[67,     1] ELBO Loss: 13.056 CE Loss: 0.010 KL Loss: 1304.674 Train Accuracy: 28.81%\n",
            "[67,    41] ELBO Loss: 514.007 CE Loss: 0.378 KL Loss: 51362.944 Train Accuracy: 30.69%\n",
            "[68,     1] ELBO Loss: 12.566 CE Loss: 0.010 KL Loss: 1255.597 Train Accuracy: 28.03%\n",
            "[68,    41] ELBO Loss: 494.503 CE Loss: 0.384 KL Loss: 49411.984 Train Accuracy: 29.49%\n",
            "[69,     1] ELBO Loss: 12.082 CE Loss: 0.009 KL Loss: 1207.249 Train Accuracy: 29.59%\n",
            "[69,    41] ELBO Loss: 475.293 CE Loss: 0.386 KL Loss: 47490.718 Train Accuracy: 29.43%\n",
            "[70,     1] ELBO Loss: 11.606 CE Loss: 0.010 KL Loss: 1159.662 Train Accuracy: 30.27%\n",
            "[70,    41] ELBO Loss: 456.397 CE Loss: 0.395 KL Loss: 45600.214 Train Accuracy: 27.48%\n",
            "[71,     1] ELBO Loss: 11.139 CE Loss: 0.010 KL Loss: 1112.861 Train Accuracy: 28.52%\n",
            "[71,    41] ELBO Loss: 437.814 CE Loss: 0.396 KL Loss: 43741.807 Train Accuracy: 27.50%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-58fa2c1df3b8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;31m# ----------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-58fa2c1df3b8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, trainloader, num_epochs, kl_weight)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.distributions import Normal, kl\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. Define Hyperparameters\n",
        "# ----------------------------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 1024\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "num_samples = 10 # Number of samples from the posterior for ELBO estimation\n",
        "kl_weight = 0.01 # Weight for the KL divergence term\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. Load and Preprocess CIFAR-10 Data\n",
        "# ----------------------------------------------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3. Define the Bayesian Linear Layer\n",
        "# ----------------------------------------------------------------------\n",
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, prior_mu=0, prior_sigma=1):\n",
        "        super(BayesianLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # Prior distribution parameters\n",
        "        self.prior_mu = prior_mu\n",
        "        self.prior_sigma = prior_sigma\n",
        "        self.prior = Normal(torch.tensor([self.prior_mu]).to(device), torch.tensor([self.prior_sigma]).to(device)) # Create a Normal distribution object\n",
        "\n",
        "        # Variational posterior parameters (learnable)\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.2, 0.2))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-5, -4)) # Initialize rho to get a small initial std\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(-0.2, 0.2))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-5, -4))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Sample weights from the variational posterior\n",
        "        weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
        "        weight_dist = Normal(self.weight_mu, weight_sigma)\n",
        "        weight = weight_dist.rsample() # Reparameterization trick\n",
        "\n",
        "        bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
        "        bias_dist = Normal(self.bias_mu, bias_sigma)\n",
        "        bias = bias_dist.rsample()  # Reparameterization trick\n",
        "\n",
        "        self.weight_log_prob = weight_dist.log_prob(weight).sum()\n",
        "        self.bias_log_prob = bias_dist.log_prob(bias).sum()\n",
        "\n",
        "        self.weight_prior_log_prob = self.prior.log_prob(weight).sum()\n",
        "        self.bias_prior_log_prob = self.prior.log_prob(bias).sum()\n",
        "\n",
        "        return nn.functional.linear(x, weight, bias)\n",
        "\n",
        "    def kl_loss(self):\n",
        "        \"\"\"Calculates the KL divergence between the variational posterior and the prior.\"\"\"\n",
        "        weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
        "        weight_dist = Normal(self.weight_mu, weight_sigma)\n",
        "        bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
        "        bias_dist = Normal(self.bias_mu, bias_sigma)\n",
        "\n",
        "        kl_weight = kl.kl_divergence(weight_dist, self.prior).sum()\n",
        "        kl_bias = kl.kl_divergence(bias_dist, self.prior).sum()\n",
        "        return kl_weight + kl_bias\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 4. Define the BNN Model\n",
        "# ----------------------------------------------------------------------\n",
        "class BNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Use BayesianLinear layers\n",
        "        self.fc1 = BayesianLinear(32 * 8 * 8, 128)  # Adjusted input size after pooling\n",
        "        self.fc2 = BayesianLinear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(nn.functional.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.dropout(x)\n",
        "        x = self.flatten(x)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def kl_loss(self):\n",
        "        \"\"\"Calculates the total KL divergence for the network.\"\"\"\n",
        "        kl_loss = self.fc1.kl_loss() + self.fc2.kl_loss()\n",
        "        return kl_loss\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5. Training Loop (MODIFIED)\n",
        "# ----------------------------------------------------------------------\n",
        "def train(model, optimizer, trainloader, num_epochs, kl_weight):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_ce_loss = 0.0  # Running Cross-Entropy Loss\n",
        "        running_kl_loss = 0.0  # Running KL Divergence Loss\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass and calculate losses\n",
        "            outputs = model(inputs)\n",
        "            log_likelihood = nn.CrossEntropyLoss()(outputs, labels) # Cross-Entropy Loss\n",
        "            kl_loss_val = model.kl_loss() # KL Divergence Loss\n",
        "            elbo = log_likelihood + kl_weight * kl_loss_val # Minimize the negative ELBO\n",
        "\n",
        "            elbo.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += elbo.item()\n",
        "            running_ce_loss += log_likelihood.item()\n",
        "            running_kl_loss += kl_loss_val.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1) # Calculate training accuracy\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # if i % 40 == 0:    # print every 200 mini-batches\n",
        "            #     avg_elbo_loss = running_loss / 200\n",
        "            #     avg_ce_loss = running_ce_loss / 200\n",
        "            #     avg_kl_loss = running_kl_loss / 200\n",
        "            #     accuracy = 100 * correct / total\n",
        "            #     print(f'[{epoch + 1}, {i + 1:5d}] ELBO Loss: {avg_elbo_loss:.3f} CE Loss: {avg_ce_loss:.3f} KL Loss: {avg_kl_loss:.3f} Train Accuracy: {accuracy:.2f}%')\n",
        "            #     running_loss = 0.0\n",
        "            #     running_ce_loss = 0.0\n",
        "            #     running_kl_loss = 0.0\n",
        "            #     correct = 0\n",
        "            #     total = 0\n",
        "\n",
        "\n",
        "        # Print epoch-level s/ummary (optional, after each epoch)\n",
        "        avg_epoch_elbo_loss = running_loss / len(trainloader)\n",
        "        avg_epoch_ce_loss = running_ce_loss / len(trainloader)\n",
        "        avg_epoch_kl_loss = running_kl_loss / len(trainloader)\n",
        "        epoch_accuracy = 100 * correct / len(trainloader.dataset)\n",
        "        print(f'Epoch [{epoch + 1}] Summary - ELBO Loss: {avg_epoch_elbo_loss:.3f} CE Loss: {avg_epoch_ce_loss:.3f} KL Loss: {avg_epoch_kl_loss:.3f} Train Accuracy: {epoch_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "# ----------------------------------------------------------------------\n",
        "# 6. Evaluation Function\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def evaluate(model, testloader, num_samples):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            # Sample multiple times from the posterior\n",
        "            predictions = torch.zeros(num_samples, images.size(0), 10).to(device) # (num_samples, batch_size, num_classes)\n",
        "\n",
        "            for i in range(num_samples):\n",
        "                outputs = model(images)\n",
        "                predictions[i] = nn.functional.softmax(outputs, dim=1)\n",
        "\n",
        "            # Average the predictions\n",
        "            mean_predictions = torch.mean(predictions, dim=0)\n",
        "            _, predicted = torch.max(mean_predictions.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f} %')\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 7.  Instantiate Model, Optimizer, and Train\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "model = BNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, optimizer, trainloader, num_epochs, kl_weight)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 8. Evaluate the Model\n",
        "# ----------------------------------------------------------------------\n",
        "evaluate(model, testloader, num_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.distributions import Normal, kl\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. Define Hyperparameters (MODIFIED)\n",
        "# ----------------------------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 1024\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "num_samples = 10 # Number of samples from the posterior for ELBO estimation\n",
        "kl_weight = 1  # Reduced KL weight to balance the loss terms\n",
        "num_train = 50000  # Number of training examples in CIFAR-10\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. Load and Preprocess CIFAR-10 Data (Unchanged)\n",
        "# ----------------------------------------------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3. Define the Bayesian Linear Layer (MODIFIED)\n",
        "# ----------------------------------------------------------------------\n",
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, prior_mu=0, prior_sigma=0.1):  # Smaller prior_sigma\n",
        "        super(BayesianLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # Prior distribution parameters\n",
        "        self.prior_mu = prior_mu\n",
        "        self.prior_sigma = prior_sigma\n",
        "        self.prior = Normal(torch.tensor([self.prior_mu]).to(device),\n",
        "                           torch.tensor([self.prior_sigma]).to(device))\n",
        "\n",
        "        # Variational posterior parameters (learnable) with higher initial rho\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.2, 0.2))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-3, -2))  # Higher rho\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(-0.2, 0.2))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-3, -2))  # Higher rho\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reparameterization\n",
        "        weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
        "        weight_dist = Normal(self.weight_mu, weight_sigma)\n",
        "        weight = weight_dist.rsample()\n",
        "\n",
        "        bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
        "        bias_dist = Normal(self.bias_mu, bias_sigma)\n",
        "        bias = bias_dist.rsample()\n",
        "\n",
        "        self.weight_log_prob = weight_dist.log_prob(weight).sum()\n",
        "        self.bias_log_prob = bias_dist.log_prob(bias).sum()\n",
        "\n",
        "        return nn.functional.linear(x, weight, bias)\n",
        "\n",
        "    def kl_loss(self):\n",
        "        weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
        "        weight_dist = Normal(self.weight_mu, weight_sigma)\n",
        "        kl_weight = kl.kl_divergence(weight_dist, self.prior).sum()\n",
        "\n",
        "        bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
        "        bias_dist = Normal(self.bias_mu, bias_sigma)\n",
        "        kl_bias = kl.kl_divergence(bias_dist, self.prior).sum()\n",
        "\n",
        "        return kl_weight + kl_bias\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 4. Define the BNN Model (Unchanged)\n",
        "# ----------------------------------------------------------------------\n",
        "class BNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = BayesianLinear(32 * 8 * 8, 128)\n",
        "        self.fc2 = BayesianLinear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(nn.functional.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.dropout(x)\n",
        "        x = self.flatten(x)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def kl_loss(self):\n",
        "        # Scale KL loss by the number of training examples\n",
        "        return (self.fc1.kl_loss() + self.fc2.kl_loss()) / num_train\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5. Training Loop (MODIFIED to use correct scaling)\n",
        "# ----------------------------------------------------------------------\n",
        "def train(model, optimizer, trainloader, num_epochs, kl_weight):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_ce_loss = 0.0\n",
        "        running_kl_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            log_likelihood = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            kl_loss_val = model.kl_loss()\n",
        "            elbo = log_likelihood + kl_weight * kl_loss_val\n",
        "\n",
        "            elbo.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += elbo.item()\n",
        "            running_ce_loss += log_likelihood.item()\n",
        "            running_kl_loss += kl_loss_val.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_accuracy = 100 * correct / total\n",
        "        avg_ce = running_ce_loss / len(trainloader)\n",
        "        avg_kl = running_kl_loss / len(trainloader)\n",
        "        avg_elbo = running_loss / len(trainloader)\n",
        "        print(f'Epoch {epoch+1}, ELBO: {avg_elbo:.3f}, CE: {avg_ce:.3f}, KL: {avg_kl:.3f}, Acc: {epoch_accuracy:.2f}%')\n",
        "    print('Finished Training')\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 6. Evaluation Function (Unchanged)\n",
        "# ----------------------------------------------------------------------\n",
        "def evaluate(model, testloader, num_samples):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            predictions = torch.zeros(num_samples, images.size(0), 10).to(device)\n",
        "            for i in range(num_samples):\n",
        "                outputs = model(images)\n",
        "                predictions[i] = nn.functional.softmax(outputs, dim=1)\n",
        "            mean_predictions = torch.mean(predictions, dim=0)\n",
        "            _, predicted = torch.max(mean_predictions.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 7. Instantiate and Train (Unchanged)\n",
        "# ----------------------------------------------------------------------\n",
        "model = BNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "train(model, optimizer, trainloader, num_epochs, kl_weight)\n",
        "evaluate(model, testloader, num_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54qhB5r0ynAJ",
        "outputId": "2d88256b-0d0c-4a8a-f141-5a85ce5e344c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, ELBO: 9.860, CE: 5.839, KL: 4.021, Acc: 13.46%\n",
            "Epoch 2, ELBO: 6.315, CE: 2.364, KL: 3.952, Acc: 13.58%\n",
            "Epoch 3, ELBO: 6.144, CE: 2.300, KL: 3.844, Acc: 13.97%\n",
            "Epoch 4, ELBO: 5.991, CE: 2.274, KL: 3.717, Acc: 15.32%\n",
            "Epoch 5, ELBO: 5.807, CE: 2.228, KL: 3.579, Acc: 16.88%\n",
            "Epoch 6, ELBO: 5.621, CE: 2.187, KL: 3.434, Acc: 19.28%\n",
            "Epoch 7, ELBO: 5.437, CE: 2.151, KL: 3.286, Acc: 20.26%\n",
            "Epoch 8, ELBO: 5.230, CE: 2.093, KL: 3.137, Acc: 22.62%\n",
            "Epoch 9, ELBO: 5.044, CE: 2.056, KL: 2.988, Acc: 24.05%\n",
            "Epoch 10, ELBO: 4.863, CE: 2.021, KL: 2.842, Acc: 25.39%\n",
            "Epoch 11, ELBO: 4.689, CE: 1.991, KL: 2.698, Acc: 26.73%\n",
            "Epoch 12, ELBO: 4.502, CE: 1.943, KL: 2.559, Acc: 28.71%\n",
            "Epoch 13, ELBO: 4.358, CE: 1.934, KL: 2.424, Acc: 29.51%\n",
            "Epoch 14, ELBO: 4.185, CE: 1.890, KL: 2.295, Acc: 30.52%\n",
            "Epoch 15, ELBO: 4.029, CE: 1.859, KL: 2.170, Acc: 32.19%\n",
            "Epoch 16, ELBO: 3.874, CE: 1.824, KL: 2.051, Acc: 33.39%\n",
            "Epoch 17, ELBO: 3.735, CE: 1.798, KL: 1.937, Acc: 34.39%\n",
            "Epoch 18, ELBO: 3.600, CE: 1.772, KL: 1.828, Acc: 35.35%\n",
            "Epoch 19, ELBO: 3.459, CE: 1.733, KL: 1.726, Acc: 37.20%\n",
            "Epoch 20, ELBO: 3.356, CE: 1.727, KL: 1.629, Acc: 37.55%\n",
            "Epoch 21, ELBO: 3.204, CE: 1.667, KL: 1.537, Acc: 39.01%\n",
            "Epoch 22, ELBO: 3.088, CE: 1.640, KL: 1.449, Acc: 40.67%\n",
            "Epoch 23, ELBO: 2.977, CE: 1.610, KL: 1.367, Acc: 40.86%\n",
            "Epoch 24, ELBO: 2.890, CE: 1.602, KL: 1.289, Acc: 41.85%\n",
            "Epoch 25, ELBO: 2.802, CE: 1.587, KL: 1.215, Acc: 42.74%\n",
            "Epoch 26, ELBO: 2.689, CE: 1.542, KL: 1.146, Acc: 43.46%\n",
            "Epoch 27, ELBO: 2.600, CE: 1.519, KL: 1.081, Acc: 44.77%\n",
            "Epoch 28, ELBO: 2.540, CE: 1.519, KL: 1.021, Acc: 45.00%\n",
            "Epoch 29, ELBO: 2.464, CE: 1.500, KL: 0.964, Acc: 45.47%\n",
            "Epoch 30, ELBO: 2.380, CE: 1.470, KL: 0.910, Acc: 46.91%\n",
            "Epoch 31, ELBO: 2.333, CE: 1.473, KL: 0.860, Acc: 46.35%\n",
            "Epoch 32, ELBO: 2.258, CE: 1.445, KL: 0.813, Acc: 47.46%\n",
            "Epoch 33, ELBO: 2.208, CE: 1.439, KL: 0.769, Acc: 48.15%\n",
            "Epoch 34, ELBO: 2.147, CE: 1.418, KL: 0.728, Acc: 48.49%\n",
            "Epoch 35, ELBO: 2.089, CE: 1.399, KL: 0.690, Acc: 49.28%\n",
            "Epoch 36, ELBO: 2.052, CE: 1.398, KL: 0.654, Acc: 49.50%\n",
            "Epoch 37, ELBO: 2.002, CE: 1.381, KL: 0.621, Acc: 49.87%\n",
            "Epoch 38, ELBO: 1.947, CE: 1.356, KL: 0.591, Acc: 50.67%\n",
            "Epoch 39, ELBO: 1.908, CE: 1.347, KL: 0.561, Acc: 51.03%\n",
            "Epoch 40, ELBO: 1.881, CE: 1.346, KL: 0.535, Acc: 50.95%\n",
            "Epoch 41, ELBO: 1.832, CE: 1.323, KL: 0.510, Acc: 52.01%\n",
            "Epoch 42, ELBO: 1.805, CE: 1.319, KL: 0.487, Acc: 52.31%\n",
            "Epoch 43, ELBO: 1.773, CE: 1.308, KL: 0.465, Acc: 52.50%\n",
            "Epoch 44, ELBO: 1.741, CE: 1.296, KL: 0.445, Acc: 53.26%\n",
            "Epoch 45, ELBO: 1.709, CE: 1.281, KL: 0.427, Acc: 53.79%\n",
            "Epoch 46, ELBO: 1.670, CE: 1.259, KL: 0.410, Acc: 54.13%\n",
            "Epoch 47, ELBO: 1.647, CE: 1.253, KL: 0.394, Acc: 54.74%\n",
            "Epoch 48, ELBO: 1.625, CE: 1.246, KL: 0.379, Acc: 55.27%\n",
            "Epoch 49, ELBO: 1.602, CE: 1.237, KL: 0.365, Acc: 55.17%\n",
            "Epoch 50, ELBO: 1.581, CE: 1.229, KL: 0.352, Acc: 55.93%\n",
            "Epoch 51, ELBO: 1.544, CE: 1.204, KL: 0.341, Acc: 56.81%\n",
            "Epoch 52, ELBO: 1.534, CE: 1.205, KL: 0.329, Acc: 56.53%\n",
            "Epoch 53, ELBO: 1.506, CE: 1.187, KL: 0.319, Acc: 57.44%\n",
            "Epoch 54, ELBO: 1.493, CE: 1.184, KL: 0.310, Acc: 57.58%\n",
            "Epoch 55, ELBO: 1.477, CE: 1.176, KL: 0.301, Acc: 58.04%\n",
            "Epoch 56, ELBO: 1.466, CE: 1.173, KL: 0.293, Acc: 57.84%\n",
            "Epoch 57, ELBO: 1.451, CE: 1.165, KL: 0.285, Acc: 58.32%\n",
            "Epoch 58, ELBO: 1.423, CE: 1.145, KL: 0.278, Acc: 59.25%\n",
            "Epoch 59, ELBO: 1.413, CE: 1.142, KL: 0.271, Acc: 59.17%\n",
            "Epoch 60, ELBO: 1.399, CE: 1.134, KL: 0.265, Acc: 59.90%\n",
            "Epoch 61, ELBO: 1.388, CE: 1.128, KL: 0.259, Acc: 59.67%\n",
            "Epoch 62, ELBO: 1.369, CE: 1.115, KL: 0.254, Acc: 60.04%\n",
            "Epoch 63, ELBO: 1.348, CE: 1.099, KL: 0.249, Acc: 60.61%\n",
            "Epoch 64, ELBO: 1.329, CE: 1.084, KL: 0.244, Acc: 61.41%\n",
            "Epoch 65, ELBO: 1.334, CE: 1.094, KL: 0.240, Acc: 60.93%\n",
            "Epoch 66, ELBO: 1.323, CE: 1.087, KL: 0.236, Acc: 61.36%\n",
            "Epoch 67, ELBO: 1.305, CE: 1.073, KL: 0.232, Acc: 61.82%\n",
            "Epoch 68, ELBO: 1.302, CE: 1.074, KL: 0.228, Acc: 61.99%\n",
            "Epoch 69, ELBO: 1.296, CE: 1.072, KL: 0.224, Acc: 62.06%\n",
            "Epoch 70, ELBO: 1.286, CE: 1.065, KL: 0.221, Acc: 62.19%\n",
            "Epoch 71, ELBO: 1.269, CE: 1.051, KL: 0.218, Acc: 62.59%\n",
            "Epoch 72, ELBO: 1.265, CE: 1.050, KL: 0.215, Acc: 63.02%\n",
            "Epoch 73, ELBO: 1.252, CE: 1.040, KL: 0.212, Acc: 63.20%\n",
            "Epoch 74, ELBO: 1.242, CE: 1.033, KL: 0.210, Acc: 63.52%\n",
            "Epoch 75, ELBO: 1.240, CE: 1.032, KL: 0.208, Acc: 63.39%\n",
            "Epoch 76, ELBO: 1.244, CE: 1.039, KL: 0.205, Acc: 63.06%\n",
            "Epoch 77, ELBO: 1.237, CE: 1.034, KL: 0.203, Acc: 63.36%\n",
            "Epoch 78, ELBO: 1.226, CE: 1.025, KL: 0.201, Acc: 63.84%\n",
            "Epoch 79, ELBO: 1.217, CE: 1.018, KL: 0.199, Acc: 63.89%\n",
            "Epoch 80, ELBO: 1.208, CE: 1.011, KL: 0.198, Acc: 64.36%\n",
            "Epoch 81, ELBO: 1.198, CE: 1.002, KL: 0.196, Acc: 64.48%\n",
            "Epoch 82, ELBO: 1.192, CE: 0.998, KL: 0.194, Acc: 64.70%\n",
            "Epoch 83, ELBO: 1.182, CE: 0.990, KL: 0.192, Acc: 65.11%\n",
            "Epoch 84, ELBO: 1.191, CE: 1.000, KL: 0.191, Acc: 64.63%\n",
            "Epoch 85, ELBO: 1.179, CE: 0.989, KL: 0.190, Acc: 64.99%\n",
            "Epoch 86, ELBO: 1.181, CE: 0.992, KL: 0.189, Acc: 65.02%\n",
            "Epoch 87, ELBO: 1.163, CE: 0.976, KL: 0.187, Acc: 65.61%\n",
            "Epoch 88, ELBO: 1.169, CE: 0.983, KL: 0.186, Acc: 65.29%\n",
            "Epoch 89, ELBO: 1.163, CE: 0.977, KL: 0.186, Acc: 65.62%\n",
            "Epoch 90, ELBO: 1.150, CE: 0.965, KL: 0.185, Acc: 65.78%\n",
            "Epoch 91, ELBO: 1.165, CE: 0.981, KL: 0.184, Acc: 65.34%\n",
            "Epoch 92, ELBO: 1.149, CE: 0.965, KL: 0.184, Acc: 66.03%\n",
            "Epoch 93, ELBO: 1.145, CE: 0.961, KL: 0.183, Acc: 66.17%\n",
            "Epoch 94, ELBO: 1.141, CE: 0.958, KL: 0.183, Acc: 66.15%\n",
            "Epoch 95, ELBO: 1.130, CE: 0.948, KL: 0.182, Acc: 66.54%\n",
            "Epoch 96, ELBO: 1.137, CE: 0.956, KL: 0.180, Acc: 66.13%\n",
            "Epoch 97, ELBO: 1.137, CE: 0.956, KL: 0.181, Acc: 66.39%\n",
            "Epoch 98, ELBO: 1.120, CE: 0.939, KL: 0.181, Acc: 66.76%\n",
            "Epoch 99, ELBO: 1.118, CE: 0.938, KL: 0.180, Acc: 66.79%\n",
            "Epoch 100, ELBO: 1.123, CE: 0.943, KL: 0.180, Acc: 66.60%\n",
            "Finished Training\n",
            "Test Accuracy: 70.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.distributions import Normal, kl\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. Define Hyperparameters (MODIFIED for MoE)\n",
        "# ----------------------------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 1024\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "num_samples = 10  # Number of samples from the posterior for ELBO estimation\n",
        "kl_weight = 1  # Reduced KL weight to balance the loss terms\n",
        "num_train = 50000  # Number of training examples in CIFAR-10\n",
        "num_experts = 3   # Number of experts in MoE\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. Load and Preprocess CIFAR-10 Data (Unchanged)\n",
        "# ----------------------------------------------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3. Define the Bayesian Linear Layer (Unchanged)\n",
        "# ----------------------------------------------------------------------\n",
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, prior_mu=0, prior_sigma=0.1):  # Smaller prior_sigma\n",
        "        super(BayesianLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # Prior distribution parameters\n",
        "        self.prior_mu = prior_mu\n",
        "        self.prior_sigma = prior_sigma\n",
        "        self.prior = Normal(torch.tensor([self.prior_mu]).to(device),\n",
        "                           torch.tensor([self.prior_sigma]).to(device))\n",
        "\n",
        "        # Variational posterior parameters (learnable) with higher initial rho\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.2, 0.2))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-3, -2))  # Higher rho\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(-0.2, 0.2))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-3, -2))  # Higher rho\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reparameterization\n",
        "        weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
        "        weight_dist = Normal(self.weight_mu, weight_sigma)\n",
        "        weight = weight_dist.rsample()\n",
        "\n",
        "        bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
        "        bias_dist = Normal(self.bias_mu, bias_sigma)\n",
        "        bias = bias_dist.rsample()\n",
        "\n",
        "        self.weight_log_prob = weight_dist.log_prob(weight).sum()\n",
        "        self.bias_log_prob = bias_dist.log_prob(bias).sum()\n",
        "\n",
        "        return nn.functional.linear(x, weight, bias)\n",
        "\n",
        "    def kl_loss(self):\n",
        "        weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
        "        weight_dist = Normal(self.weight_mu, weight_sigma)\n",
        "        kl_weight = kl.kl_divergence(weight_dist, self.prior).sum()\n",
        "\n",
        "        bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
        "        bias_dist = Normal(self.bias_mu, bias_sigma)\n",
        "        kl_bias = kl.kl_divergence(bias_dist, self.prior).sum()\n",
        "\n",
        "        return kl_weight + kl_bias\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 4. Define the Expert Network (Simple CNN Experts)\n",
        "# ----------------------------------------------------------------------\n",
        "class CNNExpert(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNExpert, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(nn.functional.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.dropout(x)\n",
        "        x = self.flatten(x)\n",
        "        return x\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5. Define the Bayesian Gating Network (BNN as Gate)\n",
        "# ----------------------------------------------------------------------\n",
        "class BayesianGatingNetwork(nn.Module):\n",
        "    def __init__(self, num_experts):\n",
        "        super(BayesianGatingNetwork, self).__init__()\n",
        "        self.expert_cnn = CNNExpert() # Shared CNN feature extractor\n",
        "        self.fc1 = BayesianLinear(32 * 8 * 8, 128)\n",
        "        self.fc2 = BayesianLinear(128, num_experts) # Output layer for gating, num_experts outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.expert_cnn(x) # Use shared CNN feature extractor\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return nn.functional.softmax(x, dim=1) # Softmax to get probabilities\n",
        "\n",
        "    def kl_loss(self):\n",
        "        return (self.fc1.kl_loss() + self.fc2.kl_loss())/num_train\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 6. Define the MoE BNN Model\n",
        "# ----------------------------------------------------------------------\n",
        "class MoE_BNN(nn.Module):\n",
        "    def __init__(self, num_experts):\n",
        "        super(MoE_BNN, self).__init__()\n",
        "        self.gate = BayesianGatingNetwork(num_experts)\n",
        "        self.experts = nn.ModuleList([nn.Sequential(\n",
        "            CNNExpert(), # Separate CNN feature extractor for each expert (can be shared or different)\n",
        "            BayesianLinear(32 * 8 * 8, 128),\n",
        "            nn.ReLU(),\n",
        "            BayesianLinear(128, 10)\n",
        "        ) for _ in range(num_experts)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate_weights = self.gate(x) # Output: [batch_size, num_experts]\n",
        "        expert_outputs = [expert(x) for expert in self.experts] # List of [batch_size, 10]\n",
        "\n",
        "        # Mixture of Experts: Weighted sum of expert outputs\n",
        "        Blended_output = torch.stack(expert_outputs, dim=2) # [batch_size, 10, num_experts]\n",
        "        Blended_output = torch.matmul(Blended_output, gate_weights.unsqueeze(2)) # [batch_size, 10, 1]\n",
        "        Blended_output = Blended_output.squeeze(2) # [batch_size, 10]\n",
        "        return Blended_output\n",
        "\n",
        "    def kl_loss(self):\n",
        "        gate_kl_loss = self.gate.kl_loss()\n",
        "        experts_kl_loss = torch.sum(torch.stack([expert[1].kl_loss() + expert[3].kl_loss() for expert in self.experts])) # Sum KL loss from all experts Bayesian layers\n",
        "        return gate_kl_loss + (experts_kl_loss)/3\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 7. Training Loop (MODIFIED for MoE BNN)\n",
        "# ----------------------------------------------------------------------\n",
        "def train(model, optimizer, trainloader, num_epochs, kl_weight):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_ce_loss = 0.0\n",
        "        running_kl_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs) # Get output from MoE_BNN\n",
        "            log_likelihood = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            kl_loss_val = model.kl_loss() / num_train # Scale KL loss by num_train\n",
        "            elbo = log_likelihood + kl_weight * kl_loss_val\n",
        "\n",
        "            elbo.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += elbo.item()\n",
        "            running_ce_loss += log_likelihood.item()\n",
        "            running_kl_loss += kl_loss_val.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_accuracy = 100 * correct / total\n",
        "        avg_ce = running_ce_loss / len(trainloader)\n",
        "        avg_kl = running_kl_loss / len(trainloader)\n",
        "        avg_elbo = running_loss / len(trainloader)\n",
        "        print(f'Epoch {epoch+1}, ELBO: {avg_elbo:.3f}, CE: {avg_ce:.3f}, KL: {avg_kl:.3f}, Acc: {epoch_accuracy:.2f}%')\n",
        "    print('Finished Training')\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 8. Evaluation Function (MODIFIED for MoE BNN)\n",
        "# ----------------------------------------------------------------------\n",
        "def evaluate(model, testloader, num_samples):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            predictions = torch.zeros(num_samples, images.size(0), 10).to(device)\n",
        "            for i in range(num_samples):\n",
        "                outputs = model(images) # Get output from MoE_BNN\n",
        "                predictions[i] = nn.functional.softmax(outputs, dim=1)\n",
        "            mean_predictions = torch.mean(predictions, dim=0)\n",
        "            _, predicted = torch.max(mean_predictions.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 9. Instantiate and Train MoE BNN\n",
        "# ----------------------------------------------------------------------\n",
        "moe_bnn_model = MoE_BNN(num_experts=num_experts).to(device)\n",
        "optimizer_moe = optim.Adam(moe_bnn_model.parameters(), lr=learning_rate)\n",
        "train(moe_bnn_model, optimizer_moe, trainloader, num_epochs, kl_weight)\n",
        "evaluate(moe_bnn_model, testloader, num_samples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlGQZ6KQvz1V",
        "outputId": "f7d2a8bb-d244-4daf-a0e8-7faabfc8df84"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1, ELBO: 8.852, CE: 4.852, KL: 4.000, Acc: 14.30%\n",
            "Epoch 2, ELBO: 6.151, CE: 2.251, KL: 3.901, Acc: 15.86%\n",
            "Epoch 3, ELBO: 5.953, CE: 2.189, KL: 3.764, Acc: 19.02%\n",
            "Epoch 4, ELBO: 5.730, CE: 2.119, KL: 3.611, Acc: 20.87%\n",
            "Epoch 5, ELBO: 5.502, CE: 2.051, KL: 3.451, Acc: 24.03%\n",
            "Epoch 6, ELBO: 5.303, CE: 2.016, KL: 3.287, Acc: 24.94%\n",
            "Epoch 7, ELBO: 5.086, CE: 1.962, KL: 3.124, Acc: 27.04%\n",
            "Epoch 8, ELBO: 4.886, CE: 1.923, KL: 2.963, Acc: 28.12%\n",
            "Epoch 9, ELBO: 4.680, CE: 1.872, KL: 2.807, Acc: 30.27%\n",
            "Epoch 10, ELBO: 4.494, CE: 1.838, KL: 2.656, Acc: 31.83%\n",
            "Epoch 11, ELBO: 4.284, CE: 1.772, KL: 2.511, Acc: 34.71%\n",
            "Epoch 12, ELBO: 4.103, CE: 1.730, KL: 2.373, Acc: 36.14%\n",
            "Epoch 13, ELBO: 3.954, CE: 1.714, KL: 2.240, Acc: 37.28%\n",
            "Epoch 14, ELBO: 3.790, CE: 1.676, KL: 2.114, Acc: 39.10%\n",
            "Epoch 15, ELBO: 3.639, CE: 1.644, KL: 1.995, Acc: 39.85%\n",
            "Epoch 16, ELBO: 3.495, CE: 1.614, KL: 1.882, Acc: 41.24%\n",
            "Epoch 17, ELBO: 3.364, CE: 1.590, KL: 1.774, Acc: 42.29%\n",
            "Epoch 18, ELBO: 3.232, CE: 1.558, KL: 1.673, Acc: 43.27%\n",
            "Epoch 19, ELBO: 3.115, CE: 1.537, KL: 1.578, Acc: 44.35%\n",
            "Epoch 20, ELBO: 2.987, CE: 1.500, KL: 1.488, Acc: 45.41%\n",
            "Epoch 21, ELBO: 2.876, CE: 1.473, KL: 1.403, Acc: 46.48%\n",
            "Epoch 22, ELBO: 2.774, CE: 1.451, KL: 1.323, Acc: 46.90%\n",
            "Epoch 23, ELBO: 2.685, CE: 1.438, KL: 1.248, Acc: 47.58%\n",
            "Epoch 24, ELBO: 2.573, CE: 1.396, KL: 1.177, Acc: 49.57%\n",
            "Epoch 25, ELBO: 2.488, CE: 1.377, KL: 1.111, Acc: 50.07%\n",
            "Epoch 26, ELBO: 2.406, CE: 1.358, KL: 1.048, Acc: 50.70%\n",
            "Epoch 27, ELBO: 2.325, CE: 1.335, KL: 0.990, Acc: 51.60%\n",
            "Epoch 28, ELBO: 2.262, CE: 1.328, KL: 0.935, Acc: 51.73%\n",
            "Epoch 29, ELBO: 2.188, CE: 1.305, KL: 0.884, Acc: 52.61%\n",
            "Epoch 30, ELBO: 2.130, CE: 1.295, KL: 0.835, Acc: 53.14%\n",
            "Epoch 31, ELBO: 2.065, CE: 1.275, KL: 0.790, Acc: 53.73%\n",
            "Epoch 32, ELBO: 2.002, CE: 1.253, KL: 0.748, Acc: 54.62%\n",
            "Epoch 33, ELBO: 1.940, CE: 1.231, KL: 0.709, Acc: 55.28%\n",
            "Epoch 34, ELBO: 1.888, CE: 1.217, KL: 0.671, Acc: 56.15%\n",
            "Epoch 35, ELBO: 1.848, CE: 1.211, KL: 0.637, Acc: 56.32%\n",
            "Epoch 36, ELBO: 1.799, CE: 1.195, KL: 0.604, Acc: 57.02%\n",
            "Epoch 37, ELBO: 1.757, CE: 1.183, KL: 0.574, Acc: 57.38%\n",
            "Epoch 38, ELBO: 1.703, CE: 1.157, KL: 0.546, Acc: 58.64%\n",
            "Epoch 39, ELBO: 1.675, CE: 1.155, KL: 0.520, Acc: 58.98%\n",
            "Epoch 40, ELBO: 1.623, CE: 1.128, KL: 0.495, Acc: 59.82%\n",
            "Epoch 41, ELBO: 1.591, CE: 1.120, KL: 0.472, Acc: 60.21%\n",
            "Epoch 42, ELBO: 1.567, CE: 1.117, KL: 0.450, Acc: 60.23%\n",
            "Epoch 43, ELBO: 1.534, CE: 1.104, KL: 0.430, Acc: 60.76%\n",
            "Epoch 44, ELBO: 1.495, CE: 1.084, KL: 0.411, Acc: 61.28%\n",
            "Epoch 45, ELBO: 1.469, CE: 1.075, KL: 0.394, Acc: 61.86%\n",
            "Epoch 46, ELBO: 1.428, CE: 1.051, KL: 0.377, Acc: 62.79%\n",
            "Epoch 47, ELBO: 1.416, CE: 1.054, KL: 0.362, Acc: 62.64%\n",
            "Epoch 48, ELBO: 1.392, CE: 1.045, KL: 0.348, Acc: 63.01%\n",
            "Epoch 49, ELBO: 1.364, CE: 1.030, KL: 0.335, Acc: 63.48%\n",
            "Epoch 50, ELBO: 1.336, CE: 1.014, KL: 0.322, Acc: 64.26%\n",
            "Epoch 51, ELBO: 1.329, CE: 1.018, KL: 0.310, Acc: 64.18%\n",
            "Epoch 52, ELBO: 1.295, CE: 0.996, KL: 0.299, Acc: 64.80%\n",
            "Epoch 53, ELBO: 1.290, CE: 1.001, KL: 0.289, Acc: 64.79%\n",
            "Epoch 54, ELBO: 1.277, CE: 0.998, KL: 0.279, Acc: 64.84%\n",
            "Epoch 55, ELBO: 1.251, CE: 0.980, KL: 0.271, Acc: 65.29%\n",
            "Epoch 56, ELBO: 1.233, CE: 0.971, KL: 0.262, Acc: 65.82%\n",
            "Epoch 57, ELBO: 1.221, CE: 0.967, KL: 0.254, Acc: 65.97%\n",
            "Epoch 58, ELBO: 1.203, CE: 0.957, KL: 0.247, Acc: 66.26%\n",
            "Epoch 59, ELBO: 1.190, CE: 0.950, KL: 0.240, Acc: 66.66%\n",
            "Epoch 60, ELBO: 1.184, CE: 0.951, KL: 0.234, Acc: 66.48%\n",
            "Epoch 61, ELBO: 1.171, CE: 0.944, KL: 0.228, Acc: 66.87%\n",
            "Epoch 62, ELBO: 1.163, CE: 0.941, KL: 0.222, Acc: 66.68%\n",
            "Epoch 63, ELBO: 1.144, CE: 0.928, KL: 0.217, Acc: 67.50%\n",
            "Epoch 64, ELBO: 1.128, CE: 0.917, KL: 0.212, Acc: 67.67%\n",
            "Epoch 65, ELBO: 1.127, CE: 0.920, KL: 0.207, Acc: 67.67%\n",
            "Epoch 66, ELBO: 1.121, CE: 0.919, KL: 0.203, Acc: 67.91%\n",
            "Epoch 67, ELBO: 1.104, CE: 0.906, KL: 0.199, Acc: 68.12%\n",
            "Epoch 68, ELBO: 1.098, CE: 0.904, KL: 0.195, Acc: 68.26%\n",
            "Epoch 69, ELBO: 1.090, CE: 0.899, KL: 0.191, Acc: 68.32%\n",
            "Epoch 70, ELBO: 1.085, CE: 0.897, KL: 0.188, Acc: 68.54%\n",
            "Epoch 71, ELBO: 1.072, CE: 0.887, KL: 0.185, Acc: 68.57%\n",
            "Epoch 72, ELBO: 1.057, CE: 0.876, KL: 0.181, Acc: 69.14%\n",
            "Epoch 73, ELBO: 1.064, CE: 0.885, KL: 0.179, Acc: 68.95%\n",
            "Epoch 74, ELBO: 1.053, CE: 0.877, KL: 0.176, Acc: 69.14%\n",
            "Epoch 75, ELBO: 1.049, CE: 0.876, KL: 0.174, Acc: 68.88%\n",
            "Epoch 76, ELBO: 1.037, CE: 0.866, KL: 0.171, Acc: 69.56%\n",
            "Epoch 77, ELBO: 1.035, CE: 0.866, KL: 0.169, Acc: 69.72%\n",
            "Epoch 78, ELBO: 1.031, CE: 0.864, KL: 0.167, Acc: 69.50%\n",
            "Epoch 79, ELBO: 1.017, CE: 0.851, KL: 0.165, Acc: 70.02%\n",
            "Epoch 80, ELBO: 1.013, CE: 0.850, KL: 0.164, Acc: 70.16%\n",
            "Epoch 81, ELBO: 1.018, CE: 0.856, KL: 0.162, Acc: 69.77%\n",
            "Epoch 82, ELBO: 1.006, CE: 0.845, KL: 0.161, Acc: 70.36%\n",
            "Epoch 83, ELBO: 0.996, CE: 0.837, KL: 0.159, Acc: 70.56%\n",
            "Epoch 84, ELBO: 1.000, CE: 0.842, KL: 0.157, Acc: 70.37%\n",
            "Epoch 85, ELBO: 0.994, CE: 0.837, KL: 0.156, Acc: 70.52%\n",
            "Epoch 86, ELBO: 0.992, CE: 0.837, KL: 0.155, Acc: 70.62%\n",
            "Epoch 87, ELBO: 0.983, CE: 0.829, KL: 0.154, Acc: 70.97%\n",
            "Epoch 88, ELBO: 0.983, CE: 0.830, KL: 0.153, Acc: 71.01%\n",
            "Epoch 89, ELBO: 0.984, CE: 0.832, KL: 0.152, Acc: 70.82%\n",
            "Epoch 90, ELBO: 0.980, CE: 0.829, KL: 0.151, Acc: 70.80%\n",
            "Epoch 91, ELBO: 0.965, CE: 0.814, KL: 0.150, Acc: 71.21%\n",
            "Epoch 92, ELBO: 0.968, CE: 0.818, KL: 0.149, Acc: 71.34%\n",
            "Epoch 93, ELBO: 0.957, CE: 0.808, KL: 0.149, Acc: 71.60%\n",
            "Epoch 94, ELBO: 0.959, CE: 0.810, KL: 0.148, Acc: 71.54%\n",
            "Epoch 95, ELBO: 0.954, CE: 0.806, KL: 0.148, Acc: 71.72%\n",
            "Epoch 96, ELBO: 0.956, CE: 0.809, KL: 0.147, Acc: 71.45%\n",
            "Epoch 97, ELBO: 0.941, CE: 0.795, KL: 0.146, Acc: 71.96%\n",
            "Epoch 98, ELBO: 0.945, CE: 0.799, KL: 0.146, Acc: 71.83%\n",
            "Epoch 99, ELBO: 0.936, CE: 0.791, KL: 0.145, Acc: 71.95%\n",
            "Epoch 100, ELBO: 0.940, CE: 0.795, KL: 0.145, Acc: 72.15%\n",
            "Finished Training\n",
            "Test Accuracy: 71.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.distributions import Normal, kl\n",
        "import numpy as np\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. Define Hyperparameters\n",
        "# ----------------------------------------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 1024\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1  # Reduced epochs for faster execution in this example\n",
        "num_samples = 10  # Number of samples from the posterior for ELBO estimation (Bayesian MoE)\n",
        "kl_weight = 1\n",
        "num_train = 50000\n",
        "num_experts = 3\n",
        "noise_std = 0.1      # Standard deviation for Gaussian noise robustness test\n",
        "epsilon_fgsm = 0.03  # Epsilon for FGSM adversarial attack\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. Load and Preprocess CIFAR-10 Data\n",
        "# ----------------------------------------------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3. Define the Bayesian Linear Layer\n",
        "# ----------------------------------------------------------------------\n",
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, prior_mu=0, prior_sigma=0.1):\n",
        "        super(BayesianLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # Prior distribution parameters\n",
        "        self.prior_mu = prior_mu\n",
        "        self.prior_sigma = prior_sigma\n",
        "        self.prior = Normal(torch.tensor([self.prior_mu]).to(device),\n",
        "                           torch.tensor([self.prior_sigma]).to(device))\n",
        "\n",
        "        # Variational posterior parameters (learnable)\n",
        "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.2, 0.2))\n",
        "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-3, -2))\n",
        "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(-0.2, 0.2))\n",
        "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-3, -2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reparameterization\n",
        "        weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
        "        weight_dist = Normal(self.weight_mu, weight_sigma)\n",
        "        weight = weight_dist.rsample()\n",
        "\n",
        "        bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
        "        bias_dist = Normal(self.bias_mu, bias_sigma)\n",
        "        bias = bias_dist.rsample()\n",
        "\n",
        "        self.weight_log_prob = weight_dist.log_prob(weight).sum()\n",
        "        self.bias_log_prob = bias_dist.log_prob(bias).sum()\n",
        "\n",
        "        return nn.functional.linear(x, weight, bias)\n",
        "\n",
        "    def kl_loss(self):\n",
        "        weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
        "        weight_dist = Normal(self.weight_mu, weight_sigma)\n",
        "        kl_weight = kl.kl_divergence(weight_dist, self.prior).sum()\n",
        "\n",
        "        bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
        "        bias_dist = Normal(self.bias_mu, bias_sigma)\n",
        "        kl_bias = kl.kl_divergence(bias_dist, self.prior).sum()\n",
        "\n",
        "        return kl_weight + kl_bias\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 4. Define the CNN Expert Network\n",
        "# ----------------------------------------------------------------------\n",
        "class CNNExpert(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNExpert, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(nn.functional.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.dropout(x)\n",
        "        x = self.flatten(x)\n",
        "        return x\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 5. Define the Bayesian Gating Network (BNN as Gate)\n",
        "# ----------------------------------------------------------------------\n",
        "class BayesianGatingNetwork(nn.Module):\n",
        "    def __init__(self, num_experts):\n",
        "        super(BayesianGatingNetwork, self).__init__()\n",
        "        self.expert_cnn = CNNExpert() # Shared CNN feature extractor\n",
        "        self.fc1 = BayesianLinear(32 * 8 * 8, 128)\n",
        "        self.fc2 = BayesianLinear(128, num_experts) # Output layer for gating, num_experts outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.expert_cnn(x) # Use shared CNN feature extractor\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return nn.functional.softmax(x, dim=1) # Softmax to get probabilities\n",
        "\n",
        "    def kl_loss(self):\n",
        "        return (self.fc1.kl_loss() + self.fc2.kl_loss())/num_train\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 6. Define the Simple Gating Network (Non-Bayesian Gate)\n",
        "# ----------------------------------------------------------------------\n",
        "class SimpleGatingNetwork(nn.Module):\n",
        "    def __init__(self, num_experts):\n",
        "        super(SimpleGatingNetwork, self).__init__()\n",
        "        self.expert_cnn = CNNExpert() # Shared CNN feature extractor\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 128) # Standard Linear Layer\n",
        "        self.fc2 = nn.Linear(128, num_experts) # Standard Linear Layer for gating outputs\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.expert_cnn(x) # Use shared CNN feature extractor\n",
        "        x = nn.functional.relu(self.fc1(x)) # ReLU activation\n",
        "        x = self.fc2(x)\n",
        "        return nn.functional.softmax(x, dim=1) # Softmax to get probabilities\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 7. Define the MoE BNN Model (Bayesian Gate)\n",
        "# ----------------------------------------------------------------------\n",
        "class MoE_BNN(nn.Module):\n",
        "    def __init__(self, num_experts):\n",
        "        super(MoE_BNN, self).__init__()\n",
        "        self.gate = BayesianGatingNetwork(num_experts)\n",
        "        self.experts = nn.ModuleList([nn.Sequential(\n",
        "            CNNExpert(), # Separate CNN feature extractor for each expert\n",
        "            BayesianLinear(32 * 8 * 8, 128),\n",
        "            nn.ReLU(),\n",
        "            BayesianLinear(128, 10)\n",
        "        ) for _ in range(num_experts)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate_weights = self.gate(x) # Output: [batch_size, num_experts]\n",
        "        expert_outputs = [expert(x) for expert in self.experts] # List of [batch_size, 10]\n",
        "\n",
        "        # Mixture of Experts: Weighted sum of expert outputs\n",
        "        Blended_output = torch.stack(expert_outputs, dim=2) # [batch_size, 10, num_experts]\n",
        "        Blended_output = torch.matmul(Blended_output, gate_weights.unsqueeze(2)) # [batch_size, 10, 1]\n",
        "        Blended_output = Blended_output.squeeze(2) # [batch_size, 10]\n",
        "        return Blended_output\n",
        "\n",
        "    def kl_loss(self):\n",
        "        gate_kl_loss = self.gate.kl_loss()\n",
        "        experts_kl_loss = torch.sum(torch.stack([expert[1].kl_loss() + expert[3].kl_loss() for expert in self.experts])) # Sum KL loss from all experts Bayesian layers\n",
        "        return gate_kl_loss + (experts_kl_loss)/3\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 8. Define the MoE with Simple Gate Model\n",
        "# ----------------------------------------------------------------------\n",
        "class MoE_SimpleGate(nn.Module):\n",
        "    def __init__(self, num_experts):\n",
        "        super(MoE_SimpleGate, self).__init__()\n",
        "        self.gate = SimpleGatingNetwork(num_experts) # Use SimpleGatingNetwork\n",
        "        self.experts = nn.ModuleList([nn.Sequential(\n",
        "            CNNExpert(), # Separate CNN feature extractor for each expert\n",
        "            nn.Linear(32 * 8 * 8, 128), # Standard Linear Layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10) # Standard Linear Layer\n",
        "        ) for _ in range(num_experts)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate_weights = self.gate(x) # Output: [batch_size, num_experts]\n",
        "        expert_outputs = [expert(x) for expert in self.experts] # List of [batch_size, 10]\n",
        "\n",
        "        # Mixture of Experts: Weighted sum of expert outputs\n",
        "        Blended_output = torch.stack(expert_outputs, dim=2) # [batch_size, 10, num_experts]\n",
        "        Blended_output = torch.matmul(Blended_output, gate_weights.unsqueeze(2)) # [batch_size, 10, 1]\n",
        "        Blended_output = Blended_output.squeeze(2) # [batch_size, 10]\n",
        "        return Blended_output\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 9. Training Function (General for both MoE types)\n",
        "# ----------------------------------------------------------------------\n",
        "def train_moe(model, optimizer, trainloader, num_epochs, kl_weight=None, is_bayesian=True): # Added is_bayesian flag\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_ce_loss = 0.0\n",
        "        running_kl_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs) # Get output from MoE\n",
        "            log_likelihood = nn.CrossEntropyLoss()(outputs, labels)\n",
        "            loss = log_likelihood # Initialize loss with CE\n",
        "\n",
        "            if is_bayesian: # Add KL loss only for Bayesian MoE\n",
        "                kl_loss_val = model.kl_loss() / num_train # Scale KL loss by num_train\n",
        "                loss = log_likelihood + kl_weight * kl_loss_val\n",
        "                running_kl_loss += kl_loss_val.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_ce_loss += log_likelihood.item()\n",
        "\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_accuracy = 100 * correct / total\n",
        "        avg_ce = running_ce_loss / len(trainloader)\n",
        "        avg_loss = running_loss / len(trainloader) # Avg Loss (ELBO or CE)\n",
        "        if is_bayesian:\n",
        "            avg_kl = running_kl_loss / len(trainloader)\n",
        "            print(f'Epoch {epoch+1}, Loss: {avg_loss:.3f} (ELBO), CE: {avg_ce:.3f}, KL: {avg_kl:.3f}, Acc: {epoch_accuracy:.2f}%')\n",
        "        else:\n",
        "            print(f'Epoch {epoch+1}, Loss: {avg_loss:.3f} (CE), CE: {avg_ce:.3f}, Acc: {epoch_accuracy:.2f}%')\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 10. Evaluation Function (General for both MoE types)\n",
        "# ----------------------------------------------------------------------\n",
        "def evaluate_moe(model, testloader, num_samples=1, is_bayesian=True): # Modified for num_samples and is_bayesian\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            if is_bayesian:\n",
        "                predictions = torch.zeros(num_samples, images.size(0), 10).to(device)\n",
        "                for i in range(num_samples):\n",
        "                    outputs = model(images) # Get output from MoE\n",
        "                    predictions[i] = nn.functional.softmax(outputs, dim=1)\n",
        "                mean_predictions = torch.mean(predictions, dim=0)\n",
        "                _, predicted = torch.max(mean_predictions.data, 1)\n",
        "            else: # For Simple Gate MoE, no sampling needed\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 11. Function to Add Gaussian Noise to Images\n",
        "# ----------------------------------------------------------------------\n",
        "def add_gaussian_noise(images, std=0.1):\n",
        "    noise = torch.randn_like(images) * std\n",
        "    noisy_images = images + noise\n",
        "    return torch.clip(noisy_images, -1, 1) # Clip to the valid range [-1, 1] for normalized CIFAR-10\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 12. Function for FGSM Adversarial Attack\n",
        "# ----------------------------------------------------------------------\n",
        "def fgsm_attack(model, images, labels, epsilon=0.03):\n",
        "    images_clone = images.clone().requires_grad_(True) # Clone and enable grad\n",
        "    outputs = model(images_clone)\n",
        "    loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "    grad_sign = images_clone.grad.data.sign()\n",
        "    adversarial_images = images + epsilon * grad_sign\n",
        "    adversarial_images = torch.clip(adversarial_images, -1, 1) # Clip to valid range\n",
        "    return adversarial_images\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 13. Evaluation for Robustness and Adversarial Attacks\n",
        "# ----------------------------------------------------------------------\n",
        "def evaluate_robustness(model, testloader, num_samples=1, is_bayesian=True, noise_std=0.1, epsilon_fgsm=0.03):\n",
        "    model.eval()\n",
        "    correct_clean = 0\n",
        "    correct_noisy = 0\n",
        "    correct_adversarial = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # --- Clean Data Evaluation ---\n",
        "            if is_bayesian:\n",
        "                predictions_clean = torch.zeros(num_samples, images.size(0), 10).to(device)\n",
        "                for i in range(num_samples):\n",
        "                    outputs_clean = model(images)\n",
        "                    predictions_clean[i] = nn.functional.softmax(outputs_clean, dim=1)\n",
        "                mean_predictions_clean = torch.mean(predictions_clean, dim=0)\n",
        "                _, predicted_clean = torch.max(mean_predictions_clean.data, 1)\n",
        "            else:\n",
        "                outputs_clean = model(images)\n",
        "                _, predicted_clean = torch.max(outputs_clean.data, 1)\n",
        "            correct_clean += (predicted_clean == labels).sum().item()\n",
        "\n",
        "\n",
        "            # --- Noisy Data Evaluation ---\n",
        "            noisy_images = add_gaussian_noise(images, std=noise_std)\n",
        "            if is_bayesian:\n",
        "                predictions_noisy = torch.zeros(num_samples, noisy_images.size(0), 10).to(device)\n",
        "                for i in range(num_samples):\n",
        "                    outputs_noisy = model(noisy_images)\n",
        "                    predictions_noisy[i] = nn.functional.softmax(outputs_noisy, dim=1)\n",
        "                mean_predictions_noisy = torch.mean(predictions_noisy, dim=0)\n",
        "                _, predicted_noisy = torch.max(mean_predictions_noisy.data, 1)\n",
        "            else:\n",
        "                outputs_noisy = model(noisy_images)\n",
        "                _, predicted_noisy = torch.max(outputs_noisy.data, 1)\n",
        "            correct_noisy += (predicted_noisy == labels).sum().item()\n",
        "\n",
        "\n",
        "            # --- Adversarial Attack Evaluation (FGSM) ---\n",
        "            adversarial_images = fgsm_attack(model, images, labels, epsilon=epsilon_fgsm)\n",
        "            if is_bayesian:\n",
        "                predictions_adv = torch.zeros(num_samples, adversarial_images.size(0), 10).to(device)\n",
        "                for i in range(num_samples):\n",
        "                    outputs_adv = model(adversarial_images)\n",
        "                    predictions_adv[i] = nn.functional.softmax(outputs_adv, dim=1)\n",
        "                mean_predictions_adv = torch.mean(predictions_adv, dim=0)\n",
        "                _, predicted_adv = torch.max(mean_predictions_adv.data, 1)\n",
        "            else:\n",
        "                outputs_adv = model(adversarial_images)\n",
        "                _, predicted_adv = torch.max(outputs_adv.data, 1)\n",
        "            correct_adversarial += (predicted_adv == labels).sum().item()\n",
        "\n",
        "            total += labels.size(0)\n",
        "\n",
        "    print(f'--- Robustness Evaluation ---')\n",
        "    print(f'Clean Accuracy:      {100 * correct_clean / total:.2f}%')\n",
        "    print(f'Noisy Accuracy (std={noise_std}): {100 * correct_noisy / total:.2f}%')\n",
        "    print(f'Adv Accuracy (FGSM, eps={epsilon_fgsm}): {100 * correct_adversarial / total:.2f}%')\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 14. Instantiate, Train and Evaluate both MoE Models\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# --- MoE with Bayesian Gate ---\n",
        "print(\"--- Training MoE with Bayesian Gate ---\")\n",
        "moe_bnn_model = MoE_BNN(num_experts=num_experts).to(device)\n",
        "optimizer_bnn_moe = optim.Adam(moe_bnn_model.parameters(), lr=learning_rate)\n",
        "train_moe(moe_bnn_model, optimizer_bnn_moe, trainloader, num_epochs, kl_weight, is_bayesian=True) # is_bayesian=True\n",
        "print(\"\\n--- Evaluation MoE with Bayesian Gate ---\")\n",
        "evaluate_robustness(moe_bnn_model, testloader, num_samples, is_bayesian=True, noise_std=noise_std, epsilon_fgsm=epsilon_fgsm) # is_bayesian=True\n",
        "\n",
        "\n",
        "# --- MoE with Simple Gate ---\n",
        "print(\"\\n--- Training MoE with Simple Gate ---\")\n",
        "moe_simple_gate_model = MoE_SimpleGate(num_experts=num_experts).to(device)\n",
        "optimizer_simple_moe = optim.Adam(moe_simple_gate_model.parameters(), lr=learning_rate)\n",
        "train_moe(moe_simple_gate_model, optimizer_simple_moe, trainloader, num_epochs, is_bayesian=False) # is_bayesian=False, no kl_weight\n",
        "print(\"\\n--- Evaluation MoE with Simple Gate ---\")\n",
        "evaluate_robustness(moe_simple_gate_model, testloader, is_bayesian=False, noise_std=noise_std, epsilon_fgsm=epsilon_fgsm) # is_bayesian=False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wHHA6DO_9s3i",
        "outputId": "6d29ccaa-027b-4cc9-a76e-5d2829ad1911"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "--- Training MoE with Bayesian Gate ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79a1d1368220>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79a1d1368220>^\n",
            "^^^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "^^    ^self._shutdown_workers()^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1587, in _shutdown_workers\n",
            "^^    ^^if w.is_alive():^\n",
            "^ ^^^^ ^  ^^ ^ ^ ^^^^^^^^\n",
            "^^^AssertionError: ^can only test a child process\n",
            "^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 9.224 (ELBO), CE: 5.233, KL: 3.992, Acc: 12.94%\n",
            "Finished Training\n",
            "\n",
            "--- Evaluation MoE with Bayesian Gate ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8f4e5405a8f5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0mtrain_moe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoe_bnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_bnn_moe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_bayesian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# is_bayesian=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Evaluation MoE with Bayesian Gate ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m \u001b[0mevaluate_robustness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoe_bnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_bayesian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_fgsm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon_fgsm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# is_bayesian=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-8f4e5405a8f5>\u001b[0m in \u001b[0;36mevaluate_robustness\u001b[0;34m(model, testloader, num_samples, is_bayesian, noise_std, epsilon_fgsm)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;31m# --- Adversarial Attack Evaluation (FGSM) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0madversarial_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfgsm_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon_fgsm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_bayesian\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mpredictions_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madversarial_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-8f4e5405a8f5>\u001b[0m in \u001b[0;36mfgsm_attack\u001b[0;34m(model, images, labels, epsilon)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m     \u001b[0mgrad_sign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages_clone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0madversarial_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_sign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5M6ICRNAIq0H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}