{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPpXiFH18dVbM35oMzzCDVj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanmhmdi/Moe-llm-edge-computing/blob/main/Untitled54.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "wZbQ7aFU0jH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 1. Network Topology for Device Simulation\n",
        "# ─────────────────────────────────────────────\n",
        "def create_network_topology():\n",
        "    \"\"\"\n",
        "    Create a simple network graph G with multiple devices.\n",
        "    Each node has a 'capacity' that represents how many 'experts' can be loaded.\n",
        "    Each edge has a 'latency' or 'cost'.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Node 0: server (large capacity to host the base LLM)\n",
        "    G.add_node(0, name=\"Server\", capacity=100)\n",
        "\n",
        "    # Additional devices\n",
        "    G.add_node(1, name=\"EdgeDevice1\", capacity=8)\n",
        "    G.add_node(2, name=\"EdgeDevice2\", capacity=8)\n",
        "\n",
        "    # Create edges with latencies\n",
        "    G.add_edge(0, 1, latency=2)\n",
        "    G.add_edge(0, 2, latency=5)\n",
        "    G.add_edge(1, 2, latency=1)\n",
        "\n",
        "    return G\n",
        "\n",
        "# A small helper that attempts to find a device in G for a given expert size\n",
        "def place_expert_on_device(G, expert_id, expert_size):\n",
        "    \"\"\"\n",
        "    Looks for a device with enough capacity to host 'expert_size'.\n",
        "    Tries the device(s) with the smallest latency to the server first.\n",
        "    Returns the chosen device id, or raises an error if none found.\n",
        "    \"\"\"\n",
        "    dist_to_server = nx.single_source_dijkstra_path_length(G, 0, weight='latency')\n",
        "    # Sort all nodes by ascending distance to server\n",
        "    device_order = sorted(list(G.nodes()), key=lambda x: dist_to_server[x])\n",
        "\n",
        "    for dev in device_order:\n",
        "        capacity = G.nodes[dev]['capacity']\n",
        "        if capacity >= expert_size:\n",
        "            G.nodes[dev]['capacity'] -= expert_size\n",
        "            return dev\n",
        "\n",
        "    raise RuntimeError(f\"Unable to place Expert {expert_id} due to insufficient capacity.\")\n",
        "\n",
        "def unload_expert_from_device(G, device_id, expert_size):\n",
        "    \"\"\"\n",
        "    Frees up capacity after the layer completes.\n",
        "    \"\"\"\n",
        "    G.nodes[device_id]['capacity'] += expert_size"
      ],
      "metadata": {
        "id": "gYXrvBTzv6IM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 2. Define a Simple Expert Submodule\n",
        "# ─────────────────────────────────────────────\n",
        "class ExpertModule(nn.Module):\n",
        "    \"\"\"\n",
        "    A small neural module that acts as an 'expert' within a layer.\n",
        "    For demonstration, we keep it trivial: a linear layer + activation.\n",
        "    In real systems, each expert might be a more complex feed-forward block.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, expert_id):\n",
        "        super().__init__()\n",
        "        self.expert_id = expert_id\n",
        "        self.fc = nn.Linear(hidden_size, hidden_size)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "        # Simulated memory usage (arbitrary). 3 means it needs capacity=3 to load\n",
        "        self.memory_footprint = 3\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        return self.activation(self.fc(hidden_states))\n",
        "\n"
      ],
      "metadata": {
        "id": "cKeKD-IOwBCU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────────────────────────────────────\n",
        "# 3. GateLayer: decides which expert to dispatch to\n",
        "# ─────────────────────────────────────────────\n",
        "class GateLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    This small gating module is added to each block. It outputs a probability\n",
        "    distribution over the available experts, effectively deciding which\n",
        "    single expert (or multiple if you extend it) is selected.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_experts=2):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        # A simple linear gating network that transforms some summary of hidden states\n",
        "        # into logits over experts. We might do something more advanced in real life.\n",
        "        self.gate_fc = nn.Linear(hidden_size, num_experts)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        \"\"\"\n",
        "        We do the gating decision on the first token only (for demonstration),\n",
        "        or you could pool the hidden states.\n",
        "        \"\"\"\n",
        "        # Suppose we look at the mean of the hidden states across sequence dimension\n",
        "        # as an input to the gate.\n",
        "        # hidden_states shape: [batch_size, seq_len, hidden_size]\n",
        "        gate_input = hidden_states.mean(dim=1)  # [batch_size, hidden_size]\n",
        "        logits = self.gate_fc(gate_input)       # [batch_size, num_experts]\n",
        "\n",
        "        # For simplicity, pick the top-1 expert for each item in the batch.\n",
        "        # In a real MoE approach, you might use top-k with dispatch logic.\n",
        "        # We'll pick one expert index across the entire batch (just for demonstration).\n",
        "        # Let’s do a simple argmax over the *first example* in the batch.\n",
        "        # shape: (num_experts,)\n",
        "        with torch.no_grad():\n",
        "            chosen_expert_idx = torch.argmax(logits[0], dim=0).item()\n",
        "\n",
        "        return chosen_expert_idx\n",
        "\n"
      ],
      "metadata": {
        "id": "yOEBH7FOv_T0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────────────────────────────────────\n",
        "# 4. A custom wrapper around the base LLM\n",
        "# ─────────────────────────────────────────────\n",
        "class MoEWrapperModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Wraps the Hugging Face model and modifies each transformer block by adding:\n",
        "      - A GateLayer that picks an expert.\n",
        "      - A set of experts within that block.\n",
        "      - A device placement simulation for the chosen expert.\n",
        "\n",
        "    This is purely conceptual to show how gating might live inside the forward pass.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_model_name, network_graph, layers_to_modify=[0,1,2], experts_per_layer=2):\n",
        "        \"\"\"\n",
        "        :param base_model_name: e.g. 'microsoft/Phi-3-mini-4k-instruct'\n",
        "        :param network_graph: The network topology for simulation\n",
        "        :param hidden_size: Model hidden size (for demonstration).\n",
        "                           Must match or be consistent with the actual base model.\n",
        "        :param layers_to_modify: Which layer indexes we insert gating + experts on.\n",
        "        :param experts_per_layer: number of experts in each modified layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.network_graph = network_graph\n",
        "\n",
        "        # 4.1) Load the HF model config, then the actual model\n",
        "        self.config = AutoConfig.from_pretrained(base_model_name)\n",
        "        # Check if hidden_size matches the model. If not, we adapt or just proceed for demonstration.\n",
        "        # For Phi-3-mini-4k-instruct, hidden size might be 1280 or something else.\n",
        "        # We'll do a smaller demonstration ignoring mismatch issues that may arise in real usage.\n",
        "        from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            trust_remote_code=True\n",
        "            # device_map=\"auto\",  # If you'd like automatic device placement\n",
        "        )\n",
        "\n",
        "        # Get the actual hidden size from the model config\n",
        "        hidden_size = self.base_model.config.hidden_size\n",
        "\n",
        "        # 4.2) For each “transformer block” we want to augment, we create:\n",
        "        #     - A GateLayer\n",
        "        #     - Some experts\n",
        "        # Because the internal structure of a HF model can vary, we won’t dive too deep into the official layer definitions.\n",
        "        # We'll store the gating modules + experts in a dictionary keyed by layer index.\n",
        "        self.layers_to_modify = layers_to_modify\n",
        "        self.experts_per_layer = experts_per_layer\n",
        "\n",
        "        self.gates = nn.ModuleDict()\n",
        "        self.experts = nn.ModuleDict()\n",
        "\n",
        "        for layer_idx in layers_to_modify:\n",
        "            self.gates[str(layer_idx)] = GateLayer(hidden_size, num_experts=experts_per_layer)\n",
        "            self.gates[str(layer_idx)].to(\"cuda\")  # Move to the desired device\n",
        "\n",
        "            # Create N experts for that layer\n",
        "            layer_experts = nn.ModuleList()\n",
        "            for e_idx in range(experts_per_layer):\n",
        "                layer_experts.append(ExpertModule(hidden_size, expert_id=e_idx))\n",
        "                layer_experts[e_idx].to(\"cuda\")  # Move to the desired device\n",
        "            self.experts[str(layer_idx)] = layer_experts\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass:\n",
        "        1) Pass tokens through each layer of the base model.\n",
        "        2) If the layer is in 'layers_to_modify', then:\n",
        "            (a) run gating\n",
        "            (b) pick the expert\n",
        "            (c) simulate loading/unloading that expert on a device\n",
        "            (d) forward hidden states through that expert\n",
        "        3) Continue to next layer.\n",
        "        4) Return final logits from the base model's output head.\n",
        "        \"\"\"\n",
        "        # (a) We first do an embedding + partial forward inside the Hugging Face model\n",
        "        #     up to each block, but this is quite complicated to do “manually.”\n",
        "        #\n",
        "        # A simpler demonstration: we use the HF model’s forward once to get hidden states,\n",
        "        # then “pretend” each layer is hooking into these hidden states.\n",
        "        # A real approach would carefully modify each block in the model’s forward pass.\n",
        "        #\n",
        "        # For demonstration, we do something approximate: get the embeddings and do\n",
        "        # a dummy multi-layer forward ourselves.\n",
        "\n",
        "        # Step 1: embed\n",
        "        # The base model usually has model.transformer.wte or similar for embeddings.\n",
        "        # Instead of picking it out precisely, we can do a partial forward pass up\n",
        "        # to the hidden states. But the “microsoft/Phi-3-mini-4k-instruct” might have\n",
        "        # a textual `forward()` that we can’t easily wrap. So let's do a simpler approach:\n",
        "        input_ids = input_ids.to(self.base_model.device)\n",
        "        embeddings = self.base_model.get_input_embeddings()(input_ids)\n",
        "\n",
        "        hidden_states = embeddings  # shape: [batch, seq_len, hidden_size]\n",
        "        # A placeholder attention mask\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "        # We'll simulate N layers. In practice, the base model might have 12 or more.\n",
        "        # For each layer, if it’s in layers_to_modify, do gating + expert forward.\n",
        "        for layer_idx in range(self.config.num_hidden_layers):\n",
        "            # If not in layers_to_modify, just do a “dummy feed-forward” so that\n",
        "            # we have some representation of continuing the forward pass\n",
        "            if layer_idx in self.layers_to_modify:\n",
        "                gate_module = self.gates[str(layer_idx)]\n",
        "                experts = self.experts[str(layer_idx)]\n",
        "\n",
        "                # 2a) gating network picks an expert index\n",
        "                chosen_expert_idx = gate_module(hidden_states.float())\n",
        "\n",
        "                # 2b) simulate device placement\n",
        "                chosen_expert = experts[chosen_expert_idx]\n",
        "                # The ID might be unique for each layer+expert, but let's demonstrate:\n",
        "                expert_unique_name = f\"layer{layer_idx}_expert{chosen_expert.expert_id}\"\n",
        "                # place expert\n",
        "                device_id = place_expert_on_device(\n",
        "                    self.network_graph,\n",
        "                    expert_unique_name,\n",
        "                    chosen_expert.memory_footprint\n",
        "                )\n",
        "\n",
        "                # 2c) forward pass through chosen expert\n",
        "                hidden_states = chosen_expert(hidden_states)\n",
        "\n",
        "                # 2d) unload from device to free capacity\n",
        "                unload_expert_from_device(\n",
        "                    self.network_graph,\n",
        "                    device_id,\n",
        "                    chosen_expert.memory_footprint\n",
        "                )\n",
        "            else:\n",
        "                # a minimal dummy layer forward\n",
        "                # in reality, we’d do the actual base model block\n",
        "                hidden_states = hidden_states + 0.01  # or some small transformation\n",
        "\n",
        "        # final LM head\n",
        "        # In real usage, we’d pass the final hidden states to base_model.lm_head\n",
        "        logits = self.base_model.lm_head(hidden_states)\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "U-YZwvY_v920"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ─────────────────────────────────────────────\n",
        "# 5. Putting It All Together\n",
        "# ─────────────────────────────────────────────\n",
        "def main():\n",
        "    # 5.1) Create the network topology\n",
        "    G = create_network_topology()\n",
        "\n",
        "    # 5.2) Build the MoE wrapper model that uses microsoft/Phi-3-mini-4k-instruct\n",
        "    #     and modifies layers 0, 1, 2 to each have 2 experts.\n",
        "    #     Note: The real Phi-3-mini-4k-instruct might differ in hidden size, etc.\n",
        "    #           This is a demonstration for the gating + device simulation concept.\n",
        "    model = MoEWrapperModel(\n",
        "        base_model_name=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "        network_graph=G,\n",
        "        hidden_size=1024,       # might need to match the actual model or be adapted\n",
        "        layers_to_modify=[0,1,2],\n",
        "        experts_per_layer=2\n",
        "    )\n",
        "\n",
        "    # 5.3) Tokenize an input\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "    input_text = \"Hello, can you explain how to use bananas and dragonfruits together?\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 5.4) Forward pass\n",
        "        # This will trigger the gating in layers 0..2, each time picking an expert\n",
        "        # and simulating device load/unload from G.\n",
        "        logits = model(**inputs)\n",
        "\n",
        "        # 5.5) Turn logits into text, just to show the flow.\n",
        "        # We'll pick the top token from logits for demonstration.\n",
        "        next_token_id = torch.argmax(logits[0, -1, :]).unsqueeze(0).unsqueeze(0)\n",
        "        generated_text = tokenizer.decode(next_token_id[0])\n",
        "        print(\"Next token predicted:\", generated_text)\n",
        "\n",
        "    # 5.6) Inspect the final device capacities to see how experts were loaded/unloaded\n",
        "    for node in G.nodes():\n",
        "        dev_name = G.nodes[node]['name']\n",
        "        curr_cap = G.nodes[node]['capacity']\n",
        "        print(f\"Device {node} ({dev_name}) final capacity: {curr_cap}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "collapsed": true,
        "id": "nDtrjjIEv8HN",
        "outputId": "c5f919bc-cdd0-4db1-b135-54a401e8ac1c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "MoEWrapperModel.__init__() got an unexpected keyword argument 'hidden_size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-aaf18389a3ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-aaf18389a3ae>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#     Note: The real Phi-3-mini-4k-instruct might differ in hidden size, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#           This is a demonstration for the gating + device simulation concept.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     model = MoEWrapperModel(\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mbase_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"microsoft/Phi-3-mini-4k-instruct\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mnetwork_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: MoEWrapperModel.__init__() got an unexpected keyword argument 'hidden_size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rglwp_uI1PMY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}