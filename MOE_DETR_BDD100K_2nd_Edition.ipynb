{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQIJmwmbe2MiAkehg2VNBf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanmhmdi/Moe-llm-edge-computing/blob/main/MOE_DETR_BDD100K_2nd_Edition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuYTqkCCXIaY",
        "outputId": "df959e15-05cf-4be2-e9de-ef5d86a4a978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/solesensei/solesensei_bdd100k?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 954M/7.61G [00:36<04:15, 28.0MB/s]"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "solesensei_solesensei_bdd100k_path = kagglehub.dataset_download('solesensei/solesensei_bdd100k')\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(solesensei_solesensei_bdd100k_path)"
      ],
      "metadata": {
        "id": "GN5Zp6iJXOUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 1) Dataset: BDD100K Detection Dataset (same as your code)\n",
        "# -------------------------------------------------------\n",
        "class BDD100KDetectionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, label_path, transforms=None, target_size=(640, 640)):\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "        self.target_size = target_size\n",
        "\n",
        "        with open(label_path) as f:\n",
        "            self.annotations = json.load(f)\n",
        "        print(f\"Initial annotations: {len(self.annotations)}\")\n",
        "\n",
        "        self.image_paths = {}\n",
        "        for root, dirs, files in os.walk(image_dir):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    key = os.path.basename(file)\n",
        "                    self.image_paths[key] = os.path.join(root, file)\n",
        "        print(f\"Discovered images: {len(self.image_paths)}\")\n",
        "\n",
        "        self.valid_annotations = []\n",
        "        for ann in self.annotations:\n",
        "            ann_filename = os.path.basename(ann['name'])\n",
        "            if ann_filename in self.image_paths:\n",
        "                self.valid_annotations.append(ann)\n",
        "        print(f\"Valid annotations after filtering: {len(self.valid_annotations)}\")\n",
        "\n",
        "        if len(self.valid_annotations) == 0:\n",
        "            raise RuntimeError(\"No valid images+annotations pairs found.\")\n",
        "\n",
        "        # BDD100K official 10 categories\n",
        "        self.CLASSES = [\n",
        "            'pedestrian', 'rider', 'car', 'truck', 'bus',\n",
        "            'train', 'motorcycle', 'bicycle', 'traffic light', 'traffic sign'\n",
        "        ]\n",
        "        self.num_obj_classes = len(self.CLASSES)\n",
        "        self.label_map = {name: idx for idx, name in enumerate(self.CLASSES)}\n",
        "\n",
        "        # By default, transform to 640x640\n",
        "        if self.transforms is None:\n",
        "            self.transforms = T.Compose([\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ann = self.valid_annotations[idx]\n",
        "        img_path = self.image_paths[ann['name']]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        original_width, original_height = image.size\n",
        "\n",
        "        # Resize with aspect ratio\n",
        "        target_width, target_height = self.target_size\n",
        "        scale = min(target_width / original_width, target_height / original_height)\n",
        "        resized_w, resized_h = int(original_width*scale), int(original_height*scale)\n",
        "        resized_image = image.resize((resized_w, resized_h), Image.BILINEAR)\n",
        "\n",
        "        # Pad to target size\n",
        "        new_image = Image.new('RGB', (target_width, target_height), (0, 0, 0))\n",
        "        new_image.paste(resized_image, (0, 0))\n",
        "        image = self.transforms(new_image)\n",
        "\n",
        "        # Create mask (False=valid, True=padding)\n",
        "        mask = torch.ones((target_height, target_width), dtype=torch.bool)\n",
        "        mask[:resized_h, :resized_w] = False\n",
        "\n",
        "        # Adjust boxes to cxcywh normalized\n",
        "        boxes = []\n",
        "        labels = []  # Initialize the labels list here\n",
        "\n",
        "        for obj in ann.get('labels', []):\n",
        "            if 'box2d' in obj and obj['category'] in self.label_map:\n",
        "                box = obj['box2d']\n",
        "                x1, y1 = box['x1'] * scale, box['y1'] * scale\n",
        "                x2, y2 = box['x2'] * scale, box['y2'] * scale\n",
        "                cx = ((x1 + x2)/2) / target_width\n",
        "                cy = ((y1 + y2)/2) / target_height\n",
        "                w = (x2 - x1) / target_width\n",
        "                h = (y2 - y1) / target_height\n",
        "                boxes.append([cx, cy, w, h])\n",
        "                labels.append(self.label_map[obj['category']])\n",
        "\n",
        "        # Convert to tensors\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,4))\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,))\n",
        "\n",
        "        target = {\n",
        "            'boxes': boxes,\n",
        "            'labels': labels,\n",
        "            'image_id': torch.tensor([idx]),\n",
        "            'mask': mask\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate to handle variable number of boxes per image.\n",
        "    Returns:\n",
        "        images: list of tensors\n",
        "        targets: list of dict\n",
        "    \"\"\"\n",
        "    return tuple(zip(*batch))\n"
      ],
      "metadata": {
        "id": "7oEaeOu2XTjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "\n",
        "\n",
        "\n",
        "class PositionEmbeddingSine(nn.Module):\n",
        "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
        "        super().__init__()\n",
        "        self.num_pos_feats = num_pos_feats\n",
        "        self.temperature = temperature\n",
        "        self.normalize = normalize\n",
        "        if scale is not None and normalize is False:\n",
        "            raise ValueError(\"normalize should be True if scale is passed\")\n",
        "        if scale is None:\n",
        "            scale = 2 * math.pi\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        assert mask is not None\n",
        "        not_mask = ~mask\n",
        "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
        "        if self.normalize:\n",
        "            eps = 1e-6\n",
        "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        return pos\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x\n",
        "\n",
        "class FrozenBatchNorm2d(torch.nn.Module):\n",
        "    def __init__(self, n):\n",
        "        super(FrozenBatchNorm2d, self).__init__()\n",
        "        self.register_buffer(\"weight\", torch.ones(n))\n",
        "        self.register_buffer(\"bias\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
        "        self.register_buffer(\"running_var\", torch.ones(n))\n",
        "\n",
        "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
        "                              missing_keys, unexpected_keys, error_msgs):\n",
        "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
        "        if num_batches_tracked_key in state_dict:\n",
        "            del state_dict[num_batches_tracked_key]\n",
        "\n",
        "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
        "            state_dict, prefix, local_metadata, strict,\n",
        "            missing_keys, unexpected_keys, error_msgs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # move reshapes to the beginning\n",
        "        # to make it fuser-friendly\n",
        "        w = self.weight.reshape(1, -1, 1, 1)\n",
        "        b = self.bias.reshape(1, -1, 1, 1)\n",
        "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
        "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
        "        eps = 1e-5\n",
        "        scale = w * (rv + eps).rsqrt()\n",
        "        bias = b - rm * scale\n",
        "        return x * scale + bias\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BoxUtils(object):\n",
        "    @staticmethod\n",
        "    def box_cxcywh_to_xyxy(x):\n",
        "        x_c, y_c, w, h = x.unbind(-1)\n",
        "        b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "             (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "        return torch.stack(b, dim=-1)\n",
        "\n",
        "    @staticmethod\n",
        "    def box_xyxy_to_cxcywh(x):\n",
        "        x0, y0, x1, y1 = x.unbind(-1)\n",
        "        b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
        "             (x1 - x0), (y1 - y0)]\n",
        "        return torch.stack(b, dim=-1)\n",
        "\n",
        "    @staticmethod\n",
        "    def rescale_bboxes(out_bbox, size):\n",
        "        img_h, img_w = size\n",
        "        b = BoxUtils.box_cxcywh_to_xyxy(out_bbox)\n",
        "        b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "        return b\n",
        "\n",
        "    @staticmethod\n",
        "    def box_area(boxes):\n",
        "        \"\"\"\n",
        "        Computes the area of a set of bounding boxes, which are specified by its\n",
        "        (x1, y1, x2, y2) coordinates.\n",
        "        Arguments:\n",
        "            boxes (Tensor[N, 4]): boxes for which the area will be computed. They\n",
        "                are expected to be in (x1, y1, x2, y2) format\n",
        "        Returns:\n",
        "            area (Tensor[N]): area for each box\n",
        "        \"\"\"\n",
        "        return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "\n",
        "    @staticmethod\n",
        "    # modified from torchvision to also return the union\n",
        "    def box_iou(boxes1, boxes2):\n",
        "        area1 = BoxUtils.box_area(boxes1)\n",
        "        area2 = BoxUtils.box_area(boxes2)\n",
        "\n",
        "        lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
        "        rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
        "\n",
        "        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "        inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
        "\n",
        "        union = area1[:, None] + area2 - inter\n",
        "\n",
        "        iou = inter / union\n",
        "        return iou, union\n",
        "\n",
        "    @staticmethod\n",
        "    def generalized_box_iou(boxes1, boxes2):\n",
        "        \"\"\"\n",
        "        Generalized IoU from https://giou.stanford.edu/\n",
        "        The boxes should be in [x0, y0, x1, y1] format\n",
        "        Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
        "        and M = len(boxes2)\n",
        "        \"\"\"\n",
        "        # degenerate boxes gives inf / nan results\n",
        "        # so do an early check\n",
        "        assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
        "        assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
        "        iou, union = BoxUtils.box_iou(boxes1, boxes2)\n",
        "\n",
        "        lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
        "        rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "\n",
        "        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "        area = wh[:, :, 0] * wh[:, :, 1]\n",
        "\n",
        "        return iou - (area - union) / area\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class HungarianMatcher(nn.Module):\n",
        "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
        "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
        "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
        "    while the others are un-matched (and thus treated as non-objects).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n",
        "        \"\"\"Creates the matcher\n",
        "        Params:\n",
        "            cost_class: This is the relative weight of the classification error in the matching cost\n",
        "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
        "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "        self.cost_giou = cost_giou\n",
        "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" Performs the matching\n",
        "        Params:\n",
        "            outputs: This is a dict that contains at least these entries:\n",
        "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
        "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
        "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
        "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
        "                           objects in the target) containing the class labels\n",
        "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
        "        Returns:\n",
        "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
        "                - index_i is the indices of the selected predictions (in order)\n",
        "                - index_j is the indices of the corresponding selected targets (in order)\n",
        "            For each batch element, it holds:\n",
        "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
        "        \"\"\"\n",
        "        bs, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "\n",
        "        # We flatten to compute the cost matrices in a batch\n",
        "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
        "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
        "\n",
        "        # Also concat the target labels and boxes\n",
        "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
        "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
        "\n",
        "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
        "        # but approximate it in 1 - proba[target class].\n",
        "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
        "        cost_class = -out_prob[:, tgt_ids]\n",
        "\n",
        "        # Compute the L1 cost between boxes\n",
        "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
        "\n",
        "        # Compute the giou cost betwen boxes\n",
        "        cost_giou = -BoxUtils.generalized_box_iou(\n",
        "            BoxUtils.box_cxcywh_to_xyxy(out_bbox),\n",
        "            BoxUtils.box_cxcywh_to_xyxy(tgt_bbox)\n",
        "        )\n",
        "\n",
        "        # Final cost matrix\n",
        "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
        "        C = C.view(bs, num_queries, -1).cpu()\n",
        "\n",
        "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
        "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
        "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\" This class computes the loss for DETR.\n",
        "    The process happens in two steps:\n",
        "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
        "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
        "        \"\"\" Create the criterion.\n",
        "        Parameters:\n",
        "            num_classes: number of object categories, omitting the special no-object category\n",
        "            matcher: module able to compute a matching between targets and proposals\n",
        "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
        "            eos_coef: relative classification weight applied to the no-object category\n",
        "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.eos_coef = eos_coef\n",
        "        self.losses = losses\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "        self.register_buffer('empty_weight', empty_weight)\n",
        "\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Classification loss (NLL)\n",
        "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
        "        \"\"\"\n",
        "        assert 'pred_logits' in outputs\n",
        "        src_logits = outputs['pred_logits']\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
        "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                    dtype=torch.int64, device=src_logits.device)\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
        "        losses = {'loss_ce': loss_ce}\n",
        "        return losses\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
        "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
        "        \"\"\"\n",
        "        pred_logits = outputs['pred_logits']\n",
        "        device = pred_logits.device\n",
        "        tgt_lengths = torch.as_tensor([len(v[\"labels\"]) for v in targets], device=device)\n",
        "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
        "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
        "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
        "        losses = {'cardinality_error': card_err}\n",
        "        return losses\n",
        "\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
        "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
        "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
        "        \"\"\"\n",
        "        assert 'pred_boxes' in outputs\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_boxes'][idx]\n",
        "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "\n",
        "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
        "\n",
        "        losses = {}\n",
        "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
        "\n",
        "        loss_giou = 1 - torch.diag(BoxUtils.generalized_box_iou(\n",
        "            BoxUtils.box_cxcywh_to_xyxy(src_boxes),\n",
        "            BoxUtils.box_cxcywh_to_xyxy(target_boxes))\n",
        "        )\n",
        "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
        "        loss_map = {\n",
        "            'labels': self.loss_labels,\n",
        "            'cardinality': self.loss_cardinality,\n",
        "            'boxes': self.loss_boxes,\n",
        "        }\n",
        "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
        "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" This performs the loss computation.\n",
        "        Parameters:\n",
        "             outputs: dict of tensors, see the output specification of the model for the format\n",
        "             targets: list of dicts, such that len(targets) == batch_size.\n",
        "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
        "        \"\"\"\n",
        "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
        "\n",
        "        # Retrieve the matching between the outputs of the last layer and the targets\n",
        "        indices = self.matcher(outputs_without_aux, targets)\n",
        "\n",
        "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
        "        num_boxes = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
        "\n",
        "        # Compute all the requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "\n",
        "class Backbone(nn.Module):\n",
        "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
        "    def __init__(self, name: str,\n",
        "                train_backbone = False,\n",
        "                pretrained = True,\n",
        "                return_interm_layers = False,\n",
        "                dilation = False,\n",
        "                norm_layer = None,\n",
        "                hidden_dim = 256):\n",
        "        super().__init__()\n",
        "        backbone = getattr(torchvision.models, name)(\n",
        "            replace_stride_with_dilation = [False, False, dilation],\n",
        "            pretrained = pretrained,\n",
        "            norm_layer = norm_layer or FrozenBatchNorm2d\n",
        "        )\n",
        "        self.num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n",
        "\n",
        "        for name, parameter in backbone.named_parameters():\n",
        "            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
        "                parameter.requires_grad_(False)\n",
        "        if return_interm_layers:\n",
        "            return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n",
        "        else:\n",
        "            return_layers = {'layer4': \"0\"}\n",
        "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
        "\n",
        "        N_steps = hidden_dim // 2\n",
        "        self.position_embedding = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        mask = mask or torch.zeros_like(x[:,0], dtype = torch.bool)\n",
        "        xs = self.body(x)\n",
        "        out: Dict[str, NestedTensor] = {}\n",
        "        for name, x in xs.items():\n",
        "            m = F.interpolate(mask[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
        "            pos = self.position_embedding(x, m).to(x.dtype)\n",
        "            out[name] = (x, m, pos)\n",
        "        return out[self.body.return_layers[\"layer4\"]]\n",
        "\n",
        "\n",
        "class ExpertMLP(nn.Module):\n",
        "    \"\"\"MLP with GELU activation for MoE experts to match transformer's original activation\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            if i < self.num_layers - 1:\n",
        "                x = F.gelu(x)\n",
        "        return x\n",
        "\n",
        "class MoE(nn.Module):\n",
        "    \"\"\"Mixture of Experts layer with top-k selection\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts, top_k):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        self.experts = nn.ModuleList([\n",
        "            ExpertMLP(input_dim, hidden_dim, output_dim, num_layers=2)\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "        self.gate = nn.Linear(input_dim, num_experts)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len, batch_size, input_dim = x.size()\n",
        "        x_flat = x.view(-1, input_dim)\n",
        "        gates = self.softmax(self.gate(x_flat))  # [seq_len*batch, num_experts]\n",
        "\n",
        "        top_k_gates, top_k_indices = torch.topk(gates, self.top_k, dim=-1)\n",
        "        top_k_gates = top_k_gates / top_k_gates.sum(dim=-1, keepdim=True)\n",
        "\n",
        "        expert_outputs = []\n",
        "        for expert in self.experts:\n",
        "            expert_outputs.append(expert(x_flat))\n",
        "        expert_outputs = torch.stack(expert_outputs, dim=1)\n",
        "\n",
        "        top_k_outputs = expert_outputs[\n",
        "            torch.arange(expert_outputs.size(0)).unsqueeze(1), top_k_indices\n",
        "        ]\n",
        "        weighted_outputs = top_k_gates.unsqueeze(-1) * top_k_outputs\n",
        "        combined = weighted_outputs.sum(dim=1).view(seq_len, batch_size, -1)\n",
        "        return combined\n",
        "\n",
        "class MoETransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"Transformer Encoder Layer with MoE\"\"\"\n",
        "    def __init__(self, d_model, nhead, num_experts, top_k, dim_feedforward=2048, dropout=0.1, activation=\"gelu\"):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.moe = MoE(d_model, dim_feedforward, d_model, num_experts, top_k)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None, is_causal=False):\n",
        "        src2 = self.self_attn(src, src, src,\n",
        "            attn_mask=src_mask,\n",
        "            key_padding_mask=src_key_padding_mask\n",
        "        )[0]\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.moe(src)\n",
        "        src = src + self.dropout2(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "class MoETransformerDecoderLayer(nn.Module):\n",
        "    \"\"\"Transformer Decoder Layer with MoE\"\"\"\n",
        "    def __init__(self, d_model, nhead, num_experts, top_k, dim_feedforward=2048, dropout=0.1, activation=\"gelu\"):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
        "        self.moe = MoE(d_model, dim_feedforward, d_model, num_experts, top_k)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
        "                tgt_key_padding_mask=None, memory_key_padding_mask=None,**kwargs):\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt,\n",
        "            attn_mask=tgt_mask,\n",
        "            key_padding_mask=tgt_key_padding_mask\n",
        "        )[0]\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory,\n",
        "            attn_mask=memory_mask,\n",
        "            key_padding_mask=memory_key_padding_mask\n",
        "        )[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.moe(tgt)\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "        return tgt\n",
        "\n",
        "class MoETransformer(nn.Module):\n",
        "    \"\"\"Transformer with MoE layers\"\"\"\n",
        "    def __init__(self, d_model=256, nhead=8, num_encoder_layers=6, num_decoder_layers=6,\n",
        "                 num_experts=4, top_k=2, dim_feedforward=2048, dropout=0.1, activation=\"gelu\"):\n",
        "        super().__init__()\n",
        "        encoder_layer = MoETransformerEncoderLayer(d_model, nhead, num_experts, top_k,\n",
        "            dim_feedforward, dropout, activation)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
        "        decoder_layer = MoETransformerDecoderLayer(d_model, nhead, num_experts, top_k,\n",
        "            dim_feedforward, dropout, activation)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None,\n",
        "                src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None, is_causal=False):\n",
        "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
        "                             tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                             memory_key_padding_mask=memory_key_padding_mask)\n",
        "        return output\n",
        "\n",
        "class DETR(nn.Module):\n",
        "    \"\"\"DETR with MoE Transformer\"\"\"\n",
        "    def __init__(self, num_classes, num_queries, backbone=None, transformer=None,\n",
        "                 num_experts=4, top_k=2):\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        if transformer is None:\n",
        "            self.transformer = MoETransformer(\n",
        "                d_model=256,\n",
        "                nhead=8,\n",
        "                num_encoder_layers=6,\n",
        "                num_decoder_layers=6,\n",
        "                num_experts=num_experts,\n",
        "                top_k=top_k\n",
        "            )\n",
        "        else:\n",
        "            self.transformer = transformer\n",
        "        hidden_dim = self.transformer.encoder.layers[0].self_attn.embed_dim\n",
        "        self.class_embed = nn.Linear(hidden_dim, num_classes + 1)\n",
        "        self.bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        self.backbone = backbone or Backbone(\n",
        "            name=\"resnet34\",\n",
        "            train_backbone=True,\n",
        "            pretrained=True,\n",
        "            return_interm_layers=True,\n",
        "            hidden_dim=hidden_dim\n",
        "        )\n",
        "        self.input_proj = nn.Conv2d(self.backbone.num_channels, hidden_dim, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, mask=None):  # Accept mask argument\n",
        "        features, mask, pos_embed = self.backbone(x)  # Pass mask to backbone\n",
        "        src = self.input_proj(features).flatten(2).permute(2, 0, 1)\n",
        "        pos_embed = pos_embed.flatten(2).permute(2, 0, 1)\n",
        "        mask = mask.flatten(1)\n",
        "        query_embed = self.query_embed.weight.unsqueeze(1).repeat(1, x.size(0), 1)\n",
        "        tgt = torch.zeros_like(query_embed)\n",
        "        hs = self.transformer(\n",
        "            src=src + pos_embed,\n",
        "            tgt=tgt + query_embed,\n",
        "            src_key_padding_mask=mask,\n",
        "            memory_key_padding_mask=mask\n",
        "        )\n",
        "        outputs_class = self.class_embed(hs)\n",
        "        outputs_coord = self.bbox_embed(hs).sigmoid()\n",
        "        return {\n",
        "            'pred_logits': outputs_class.permute(1, 0, 2),\n",
        "            'pred_boxes': outputs_coord.permute(1, 0, 2)\n",
        "        }\n",
        "\n",
        "\n",
        "num_classes = 5\n",
        "num_queries = 100\n",
        "model = DETR(num_classes, num_queries, num_experts=8, top_k=2)\n",
        "\n",
        "matcher = HungarianMatcher(cost_class = 1, cost_bbox = 5, cost_giou = 2)\n",
        "weight_dict = {'loss_ce': 1, 'loss_bbox': 5, 'loss_giou': 2}\n",
        "losses = ['labels', 'boxes', 'cardinality']\n",
        "criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict, eos_coef=0.1, losses=losses)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hwyHc-NIXTlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_integration_check():\n",
        "\n",
        "    num_classes = 5\n",
        "    num_queries = 100\n",
        "    model = DETR(num_classes, num_queries, num_experts=8, top_k=2)\n",
        "\n",
        "    matcher = HungarianMatcher(cost_class = 1, cost_bbox = 5, cost_giou = 2)\n",
        "    weight_dict = {'loss_ce': 1, 'loss_bbox': 5, 'loss_giou': 2}\n",
        "    losses = ['labels', 'boxes', 'cardinality']\n",
        "    criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict, eos_coef=0.1, losses=losses)\n",
        "\n",
        "\n",
        "    # Test configuration\n",
        "    TEST_IMAGE_DIR = \"/root/.cache/kagglehub/datasets/solesensei/solesensei_bdd100k/versions/2/bdd100k/bdd100k/images/100k/val\"  # Update with actual path\n",
        "    TEST_LABEL_PATH = \"/root/.cache/kagglehub/datasets/solesensei/solesensei_bdd100k/versions/2/bdd100k_labels_release/bdd100k/labels/bdd100k_labels_images_val.json\"  # Update with actual path\n",
        "\n",
        "    BATCH_SIZE = 2\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # 1. Test dataset initialization\n",
        "    print(\"Initializing dataset...\")\n",
        "    dataset = BDD100KDetectionDataset(\n",
        "        image_dir=TEST_IMAGE_DIR,\n",
        "        label_path=TEST_LABEL_PATH,\n",
        "        target_size=(640, 640)\n",
        "    )\n",
        "    print(f\"Dataset contains {len(dataset)} samples\")\n",
        "\n",
        "    # 2. Test dataloader\n",
        "    print(\"\\nTesting dataloader...\")\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda batch: tuple(zip(*batch)))\n",
        "\n",
        "    # 3. Verify batch shapes\n",
        "    images, targets = next(iter(dataloader))\n",
        "    print(f\"\\nBatch contains {len(images)} images\")\n",
        "    print(f\"Image tensor shape: {images[0].shape}\")  # Should be [3, 640, 640]\n",
        "    print(f\"Number of targets in first sample: {len(targets[0]['boxes'])}\")\n",
        "\n",
        "    # 4. Verify mask creation\n",
        "    print(\"\\nTesting mask creation:\")\n",
        "    mask = targets[0]['mask']\n",
        "    print(f\"Mask shape: {mask.shape}\")  # Should be [640, 640]\n",
        "    print(f\"Mask dtype: {mask.dtype}\")   # Should be torch.bool\n",
        "    print(f\"Padding area ratio: {mask.float().mean().item():.2f}\")  # Should be < 1.0\n",
        "\n",
        "    # 5. Verify box normalization\n",
        "    print(\"\\nTesting box normalization:\")\n",
        "    boxes = targets[0]['boxes']\n",
        "    if len(boxes) > 0:\n",
        "        print(f\"First box coordinates: {boxes[0].tolist()}\")\n",
        "        print(\"All coordinates should be between 0-1:\")\n",
        "        print(f\"X range: {boxes[:,0].min().item():.2f}-{boxes[:,0].max().item():.2f}\")\n",
        "        print(f\"Y range: {boxes[:,1].min().item():.2f}-{boxes[:,1].max().item():.2f}\")\n",
        "        print(f\"W range: {boxes[:,2].min().item():.2f}-{boxes[:,2].max().item():.2f}\")\n",
        "        print(f\"H range: {boxes[:,3].min().item():.2f}-{boxes[:,3].max().item():.2f}\")\n",
        "\n",
        "    # 6. Test model forward pass\n",
        "    print(\"\\nTesting model forward pass...\")\n",
        "    model = DETR(num_classes=10, num_queries=100, num_experts=8, top_k=2).to(DEVICE)\n",
        "    criterion = SetCriterion(10, HungarianMatcher(), weight_dict, eos_coef=0.1, losses=losses)\n",
        "\n",
        "    # Convert images to tensor batch\n",
        "    images = [img.to(DEVICE) for img in images]\n",
        "    images = torch.stack(images)\n",
        "    masks = [t['mask'].to(DEVICE) for t in targets]\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images, masks)\n",
        "    print(\"Model outputs shapes:\")\n",
        "    print(f\"Logits: {outputs['pred_logits'].shape}\")  # Should be [B, 100, 11]\n",
        "    print(f\"Boxes: {outputs['pred_boxes'].shape}\")    # Should be [B, 100, 4]\n",
        "\n",
        "    # 7. Test loss calculation\n",
        "    print(\"\\nTesting loss calculation...\")\n",
        "    targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
        "    losses = criterion(outputs, targets)\n",
        "    total_loss = sum(losses.values())\n",
        "    print(f\"Total loss: {total_loss.item():.4f}\")\n",
        "    print(\"Individual losses:\")\n",
        "    for name, loss in losses.items():\n",
        "        print(f\"{name}: {loss.item():.4f}\")\n",
        "\n",
        "    print(\"\\nAll checks passed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_integration_check()"
      ],
      "metadata": {
        "id": "ntCvjFVaXTn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Assume that the following classes have been defined/imported from your code:\n",
        "# - BDD100KDetectionDataset\n",
        "# - collate_fn\n",
        "# - DETR\n",
        "# - HungarianMatcher\n",
        "# - SetCriterion\n",
        "\n",
        "def train_one_epoch(model, criterion, optimizer, dataloader, device, scaler, epoch):\n",
        "    model.train()\n",
        "    criterion.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\", unit=\"batch\")\n",
        "    for images, targets in pbar:\n",
        "        # Move images to device and stack into a single tensor\n",
        "        images = [img.to(device) for img in images]\n",
        "        images = torch.stack(images)\n",
        "\n",
        "        # Extract and move masks from targets\n",
        "        masks = [t['mask'].to(device) for t in targets]\n",
        "\n",
        "        # Move all target tensors to the device\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            outputs = model(images, masks)  # Forward pass (pass masks if needed)\n",
        "            loss_dict = criterion(outputs, targets)\n",
        "            loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    return epoch_loss\n",
        "\n",
        "def main():\n",
        "    # ---------------------------\n",
        "    # Hyperparameters and paths\n",
        "    # ---------------------------\n",
        "    num_epochs = 10\n",
        "    learning_rate = 1e-4\n",
        "    batch_size = 2\n",
        "\n",
        "    # Update these paths with the actual locations of your images and labels.\n",
        "    IMAGE_DIR = \"/root/.cache/kagglehub/datasets/solesensei/solesensei_bdd100k/versions/2/bdd100k/bdd100k/images/100k/val\"\n",
        "    LABEL_PATH = \"/root/.cache/kagglehub/datasets/solesensei/solesensei_bdd100k/versions/2/bdd100k_labels_release/bdd100k/labels/bdd100k_labels_images_val.json\"\n",
        "\n",
        "    # ---------------------------\n",
        "    # Dataset and Dataloader\n",
        "    # ---------------------------\n",
        "    dataset = BDD100KDetectionDataset(\n",
        "        image_dir=IMAGE_DIR,\n",
        "        label_path=LABEL_PATH,\n",
        "        target_size=(640, 640)\n",
        "    )\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # ---------------------------\n",
        "    # Model, Criterion, Optimizer\n",
        "    # ---------------------------\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Set the number of classes and queries. (Update num_classes if needed)\n",
        "    num_classes = 10\n",
        "    num_queries = 100\n",
        "\n",
        "    model = DETR(num_classes, num_queries, num_experts=8, top_k=2).to(device)\n",
        "\n",
        "    matcher = HungarianMatcher(cost_class=1, cost_bbox=5, cost_giou=2)\n",
        "    weight_dict = {'loss_ce': 1, 'loss_bbox': 5, 'loss_giou': 2}\n",
        "    losses = ['labels', 'boxes', 'cardinality']\n",
        "    criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict, eos_coef=0.1, losses=losses).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # For mixed precision training\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # ---------------------------\n",
        "    # Training Loop\n",
        "    # ---------------------------\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        epoch_loss = train_one_epoch(model, criterion, optimizer, dataloader, device, scaler, epoch)\n",
        "        print(f\"Epoch {epoch} Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        # Optionally, save checkpoints:\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': epoch_loss\n",
        "        }\n",
        "        torch.save(checkpoint, f'checkpoint_epoch_{epoch}.pth')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "nc3wAZW8XTsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GS0VcRuHXTvb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}