{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNa19qBcAR9GO0oasju69mF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanmhmdi/Moe-llm-edge-computing/blob/main/moe_classification_switch_transformer_placment_model_newtork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w42BfQzfGsBb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ------------------------------\n",
        "# 1. DataLoader for MNIST\n",
        "# ------------------------------\n",
        "def get_data_loaders(batch_size=64):\n",
        "    \"\"\"\n",
        "    Returns train and test DataLoader for MNIST dataset.\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 2. MoE Components\n",
        "# ------------------------------\n",
        "\n",
        "class GatingNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A gating network that outputs a distribution over experts\n",
        "    and the top-k selected expert indices.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, num_experts, top_k=1):\n",
        "        super(GatingNetwork, self).__init__()\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(in_channels, num_experts)\n",
        "        self.top_k = top_k\n",
        "        # We'll store the last top_k indices for external use (placement training)\n",
        "        self.last_topk_indices = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        pooled = self.global_pool(x).view(batch_size, -1)\n",
        "        logits = self.fc(pooled)  # shape: (batch_size, num_experts)\n",
        "\n",
        "        topk_logits, topk_indices = torch.topk(logits, self.top_k, dim=1)\n",
        "        topk_weights = F.softmax(topk_logits, dim=1)\n",
        "\n",
        "        # Create a sparse weights tensor (batch_size, num_experts) with zeros everywhere\n",
        "        sparse_weights = torch.zeros_like(logits)\n",
        "        sparse_weights.scatter_(1, topk_indices, topk_weights)\n",
        "\n",
        "        # Store these indices for later usage (placement)\n",
        "        self.last_topk_indices = topk_indices.detach()\n",
        "\n",
        "        return sparse_weights, topk_indices\n",
        "\n",
        "\n",
        "class Expert(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple CNN expert: Conv -> BN -> ReLU.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Expert, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "\n",
        "class MoEBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Mixture-of-Experts block:\n",
        "     - Gating network to select top_k experts\n",
        "     - Weighted sum of expert outputs\n",
        "     - Optional load-balance calculation\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, num_experts=4, top_k=1):\n",
        "        super(MoEBlock, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        self.experts = nn.ModuleList([Expert(in_channels, out_channels)\n",
        "                                      for _ in range(num_experts)])\n",
        "        self.gate = GatingNetwork(in_channels, num_experts, top_k=top_k)\n",
        "        self.out_conv = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # gating\n",
        "        gate_weights, topk_indices = self.gate(x)\n",
        "\n",
        "        # Compute each expert’s output\n",
        "        expert_outputs = [expert(x) for expert in self.experts]\n",
        "        # Stack along a new dimension => shape: (batch, num_experts, C, H, W)\n",
        "        expert_stack = torch.stack(expert_outputs, dim=1)\n",
        "        # Reshape gate weights => (batch, num_experts, 1, 1, 1)\n",
        "        gate_weights = gate_weights.view(-1, self.num_experts, 1, 1, 1)\n",
        "        # Weighted sum\n",
        "        out = (expert_stack * gate_weights).sum(dim=1)\n",
        "\n",
        "        # A little post-processing\n",
        "        out = self.relu(self.bn(self.out_conv(out)))\n",
        "\n",
        "        # (Optional) load balance loss or usage stats\n",
        "        # We'll define them as placeholders so the signature remains\n",
        "        batch_size = x.size(0)\n",
        "        importance = gate_weights.squeeze().sum(0)\n",
        "        diversity = (gate_weights.squeeze() > 0).float().sum(0)\n",
        "        mean_importance = importance / batch_size\n",
        "        mean_diversity = diversity / batch_size\n",
        "        load_balance_loss = (mean_importance * mean_diversity).sum()\n",
        "\n",
        "        # Expert distribution\n",
        "        flat_indices = topk_indices.view(-1)\n",
        "        counts = torch.bincount(flat_indices, minlength=self.num_experts)\n",
        "        proportions = counts.float() / (batch_size * self.top_k)\n",
        "\n",
        "        return out, load_balance_loss, proportions\n",
        "\n",
        "\n",
        "class MoEClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple 2-layer MoE CNN for MNIST classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, num_experts=4, top_k=1):\n",
        "        super(MoEClassifier, self).__init__()\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        self.moe_block1 = MoEBlock(32, 64, num_experts, top_k)\n",
        "        self.moe_block2 = MoEBlock(64, 128, num_experts, top_k)\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lb_loss = 0.0\n",
        "        gate_dists = []\n",
        "\n",
        "        x = self.initial(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x, lb1, dist1 = self.moe_block1(x)\n",
        "        lb_loss += lb1\n",
        "        gate_dists.append(dist1)\n",
        "\n",
        "        x = self.pool(x)\n",
        "        x, lb2, dist2 = self.moe_block2(x)\n",
        "        lb_loss += lb2\n",
        "        gate_dists.append(dist2)\n",
        "\n",
        "        x = self.global_pool(x).view(x.size(0), -1)\n",
        "        logits = self.fc(x)\n",
        "\n",
        "        return logits, lb_loss, gate_dists\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 3. Training and Testing (Base Model)\n",
        "# ------------------------------\n",
        "def train_base_model(model, device, train_loader, optimizer, criterion, epoch, aux_coeff):\n",
        "    \"\"\"\n",
        "    Train loop for the MoE base model.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, aux_loss, gate_dists = model(data)\n",
        "        loss = criterion(logits, target) + aux_coeff * aux_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = logits.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}]\")\n",
        "            print(f\"  Loss: {loss.item():.6f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100.0 * correct / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch} - Base Model Train Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "def tes_base_model(model, device, test_loader, criterion):\n",
        "    \"\"\"\n",
        "    Test loop for the MoE base model.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # For distribution stats\n",
        "    gate1_total = torch.zeros(model.moe_block1.num_experts).to(device)\n",
        "    gate2_total = torch.zeros(model.moe_block2.num_experts).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            batch_size = data.size(0)\n",
        "            logits, _, gate_dists = model(data)\n",
        "\n",
        "            loss = criterion(logits, target)\n",
        "            test_loss += loss.item()\n",
        "            pred = logits.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total_samples += batch_size\n",
        "\n",
        "            # Accumulate gating distribution\n",
        "            gate1_total += gate_dists[0] * batch_size\n",
        "            gate2_total += gate_dists[1] * batch_size\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = 100.0 * correct / total_samples\n",
        "    print(f\"Base Model Test - Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Print average distribution\n",
        "    gate1_avg = (gate1_total / total_samples).cpu().numpy()\n",
        "    gate2_avg = (gate2_total / total_samples).cpu().numpy()\n",
        "    print(\"Block1 Dist (%):\", \" \".join([f\"{100*d:.1f}\" for d in gate1_avg]))\n",
        "    print(\"Block2 Dist (%):\", \" \".join([f\"{100*d:.1f}\" for d in gate2_avg]))\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 4. Network Topology (Server <-> Devices)\n",
        "# ------------------------------\n",
        "class NetworkTopology:\n",
        "    \"\"\"\n",
        "    Minimal example: server is ID=0, devices are 1..N.\n",
        "    'comm_cost[i]' is cost from server to device i.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_devices=3, base_cost=5.0):\n",
        "        self.num_devices = num_devices\n",
        "        # cost[0] for server => 0\n",
        "        # cost[i] for device i => base_cost + small random\n",
        "        self.comm_cost = [0.0]\n",
        "        for i in range(1, num_devices+1):\n",
        "            cost_i = base_cost + np.random.rand() * 2.0\n",
        "            self.comm_cost.append(cost_i)\n",
        "\n",
        "    def get_cost(self, device_id):\n",
        "        return self.comm_cost[device_id]\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 5. Placement Selector Model\n",
        "# ------------------------------\n",
        "class PlacementSelector(nn.Module):\n",
        "    \"\"\"\n",
        "    Predicts device placements for each selected expert.\n",
        "    For demonstration: uses an embedding of (layer_id, expert_id) to\n",
        "    produce a distribution over device_id in [0..num_devices].\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, num_experts, num_devices=3, embedding_dim=16):\n",
        "        super(PlacementSelector, self).__init__()\n",
        "        self.num_devices = num_devices\n",
        "        # total_expert_keys = num_layers * num_experts\n",
        "        self.num_layers = num_layers\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        # We create a single embedding to handle (layer_id, expert_id) => index\n",
        "        self.embedding = nn.Embedding(num_layers * num_experts, embedding_dim)\n",
        "        # Output distribution over [0..num_devices], i.e. (num_devices+1) possibilities\n",
        "        self.fc = nn.Linear(embedding_dim, num_devices+1)\n",
        "\n",
        "    def forward(self, layer_ids, expert_ids):\n",
        "        \"\"\"\n",
        "        :param layer_ids:   (batch, top_k)\n",
        "        :param expert_ids:  (batch, top_k)\n",
        "        :return: device_probs => (batch, top_k, num_devices+1)\n",
        "        \"\"\"\n",
        "        # Convert (layer_id, expert_id) to single index\n",
        "        # index = layer_id * num_experts + expert_id\n",
        "        combined_key = layer_ids * self.num_experts + expert_ids\n",
        "        flat_key = combined_key.view(-1)\n",
        "        emb = self.embedding(flat_key)  # shape: (batch*top_k, embedding_dim)\n",
        "        logits = self.fc(emb)           # shape: (batch*top_k, num_devices+1)\n",
        "        device_probs = F.softmax(logits, dim=-1)\n",
        "        # Reshape\n",
        "        B, K = combined_key.shape\n",
        "        device_probs = device_probs.view(B, K, -1)\n",
        "        return device_probs\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 6. Communication-Cost–Based Loss\n",
        "# ------------------------------\n",
        "def compute_placement_loss(device_probs, topology):\n",
        "    \"\"\"\n",
        "    Simple version: we pick argmax as the device assignment (non-differentiable).\n",
        "    Then cost = topology.get_cost(assigned_device).\n",
        "    best_cost = minimal cost among all devices.\n",
        "    Loss = chosen_cost - best_cost, averaged over all samples.\n",
        "    \"\"\"\n",
        "    batch_size, top_k, num_dev_plus_one = device_probs.size()\n",
        "    chosen_device = torch.argmax(device_probs, dim=-1)  # shape: (batch_size, top_k)\n",
        "\n",
        "    total_diff = 0.0\n",
        "    # We'll do a simple loop for clarity\n",
        "    for b in range(batch_size):\n",
        "        for k in range(top_k):\n",
        "            d_id = chosen_device[b, k].item()  # integer device ID\n",
        "            chosen_cost = topology.get_cost(d_id)\n",
        "            # best possible cost\n",
        "            best_cost = min(topology.get_cost(d) for d in range(num_dev_plus_one))\n",
        "            total_diff += (chosen_cost - best_cost)\n",
        "\n",
        "    avg_loss = total_diff / (batch_size * top_k)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 7. Train the Placement Selector\n",
        "# ------------------------------\n",
        "def train_placement_selector(base_model, placement_model, device,\n",
        "                             train_loader, optimizer, topology, num_layers):\n",
        "    \"\"\"\n",
        "    Only the placement_model is trained. The base_model is frozen.\n",
        "    We'll collect top-k indices from each MoEBlock's gating.\n",
        "    \"\"\"\n",
        "    base_model.eval()\n",
        "    placement_model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "\n",
        "    # We expect 2 MoE blocks => layer_ids: block1->0, block2->1 (for example)\n",
        "    # Adjust if more layers are present.\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        _ = target.to(device)  # we don't actually need target for cost-based training\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass (frozen) to get top-k indices from each block\n",
        "        with torch.no_grad():\n",
        "            _ = base_model(data)  # This populates gating networks with .last_topk_indices\n",
        "\n",
        "        # For block1\n",
        "        topk_indices_block1 = base_model.moe_block1.gate.last_topk_indices  # shape: (batch, top_k)\n",
        "        layer_ids_block1 = torch.zeros_like(topk_indices_block1, device=device)  # fill with 0\n",
        "\n",
        "        # For block2\n",
        "        topk_indices_block2 = base_model.moe_block2.gate.last_topk_indices\n",
        "        layer_ids_block2 = torch.ones_like(topk_indices_block2, device=device)  # fill with 1\n",
        "\n",
        "        # Placement model forward\n",
        "        device_probs_block1 = placement_model(layer_ids_block1, topk_indices_block1)\n",
        "        device_probs_block2 = placement_model(layer_ids_block2, topk_indices_block2)\n",
        "\n",
        "        # Compute cost-based loss\n",
        "        loss_block1 = compute_placement_loss(device_probs_block1, topology)\n",
        "        loss_block2 = compute_placement_loss(device_probs_block2, topology)\n",
        "        loss = loss_block1 + loss_block2\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        count += 1\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"[{batch_idx}/{len(train_loader)}] Placement Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / max(1, count)\n",
        "    print(f\"Placement Training - Avg Cost Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 8. Main: Train base model, then freeze & train placement\n",
        "# ------------------------------\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Prepare data\n",
        "    train_loader, test_loader = get_data_loaders(batch_size=64)\n",
        "\n",
        "    # -------------------------\n",
        "    # A) Train the base MoE model\n",
        "    # -------------------------\n",
        "    num_experts = 8\n",
        "    top_k = 2\n",
        "    base_model = MoEClassifier(num_experts=num_experts, top_k=top_k).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(base_model.parameters(), lr=0.001)\n",
        "    aux_coeff = 2.0\n",
        "\n",
        "    print(\"\\n--- Training Base MoE Model ---\")\n",
        "    base_epochs = 2  # keep it small for demo\n",
        "    for epoch in range(1, base_epochs+1):\n",
        "        train_base_model(base_model, device, train_loader, optimizer, criterion, epoch, aux_coeff)\n",
        "        tes_base_model(base_model, device, test_loader, criterion)\n",
        "\n",
        "    # Save the trained model\n",
        "    base_model_path = \"moe_mnist_balanced.pth\"\n",
        "    torch.save(base_model.state_dict(), base_model_path)\n",
        "    print(f\"Base model saved to {base_model_path}.\")\n",
        "\n",
        "    # -------------------------\n",
        "    # B) Load the pretrained MoE in a frozen state\n",
        "    # -------------------------\n",
        "    frozen_model = MoEClassifier(num_experts=num_experts, top_k=top_k).to(device)\n",
        "    frozen_model.load_state_dict(torch.load(base_model_path, map_location=device))\n",
        "\n",
        "    for param in frozen_model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # -------------------------\n",
        "    # C) Create a topology and the PlacementSelector\n",
        "    # -------------------------\n",
        "    num_devices = 3  # number of edge devices\n",
        "    topology = NetworkTopology(num_devices=num_devices, base_cost=5.0)\n",
        "\n",
        "    # We have 2 MoE blocks => num_layers=2 in the base model\n",
        "    placement_model = PlacementSelector(num_layers=2,\n",
        "                                       num_experts=num_experts,\n",
        "                                       num_devices=num_devices,\n",
        "                                       embedding_dim=16).to(device)\n",
        "\n",
        "    placement_optimizer = optim.Adam(placement_model.parameters(), lr=1e-3)\n",
        "\n",
        "    # -------------------------\n",
        "    # D) Train the placement selector\n",
        "    # -------------------------\n",
        "    print(\"\\n--- Training Placement Selector ---\")\n",
        "    placement_epochs = 3\n",
        "    for epoch in range(1, placement_epochs+1):\n",
        "        print(f\"=== Placement Epoch {epoch} ===\")\n",
        "        train_placement_selector(frozen_model, placement_model, device,\n",
        "                                 train_loader, placement_optimizer,\n",
        "                                 topology, num_layers=2)\n",
        "\n",
        "    # Save the trained placement model\n",
        "    placement_path = \"placement_selector.pth\"\n",
        "    torch.save(placement_model.state_dict(), placement_path)\n",
        "    print(f\"Placement selector saved to {placement_path}.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Make sure we're not cluttering with old plots, etc.\n",
        "    # You can remove or comment out these lines if desired\n",
        "    if not os.path.exists(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "laTvYrdJGtA6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}